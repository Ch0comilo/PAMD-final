{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465a3b20",
   "metadata": {},
   "source": [
    "### Verificación del Entorno MLOps\n",
    "\n",
    "Este documento te guiará para validar que todos los servicios de tu infraestructura MLOps (Jupyter, Spark, MLflow, Kafka y Airflow) están activos y se comunican correctamente.\n",
    "\n",
    "#### 1\\. Verificar Contenedores Activos\n",
    "\n",
    "Primero, asegúrate de que todos los contenedores definidos en tu `docker-compose.yml` estén corriendo.\n",
    "\n",
    "  * **Comando:**\n",
    "\n",
    "    ```bash\n",
    "    docker ps\n",
    "    ```\n",
    "\n",
    "  * **Resultado Esperado:** Deberías ver una lista con contenedores para los siguientes servicios (los nombres pueden variar ligeramente según tu configuración):\n",
    "\n",
    "      * `python_ml_stack` (Jupyter/Spark/MLflow)\n",
    "      * `postgres_db`\n",
    "      * `kafka`\n",
    "      * `zookeeper`\n",
    "      * `airflow_webserver`\n",
    "      * `airflow_scheduler`\n",
    "\n",
    "Si algún contenedor no está en la lista o su estado es `Exited`, revisa los logs con `docker logs <nombre_contenedor>`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0435292c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### 2\\. Acceder a las Interfaces Web (UIs)\n",
    "\n",
    "Verifica que puedes acceder a las interfaces gráficas de los servicios desde tu navegador.\n",
    "\n",
    "  * **JupyterLab:**\n",
    "\n",
    "      * URL: `http://localhost:8888`\n",
    "      * Token: El que definiste en tu `Dockerfile` (`hola-mundo` según tu configuración actual) o el que aparece en los logs de inicio.\n",
    "      * **Prueba:** Intenta crear un nuevo notebook.\n",
    "\n",
    "  * **Spark Master UI:**\n",
    "\n",
    "      * URL: `http://localhost:8080`\n",
    "      * **Prueba:** Verifica que aparezca el Worker conectado y que haya recursos disponibles (Cores/Memory).\n",
    "\n",
    "  * **MLflow UI:**\n",
    "\n",
    "      * URL: `http://localhost:5000`\n",
    "      * **Prueba:** Deberías ver la interfaz de MLflow. Intenta crear un experimento de prueba desde un notebook (ver sección 4).\n",
    "\n",
    "  * **Airflow Webserver:**\n",
    "\n",
    "      * URL: `http://localhost:8081`\n",
    "      * Credenciales: `admin` / `admin` (según las variables de entorno configuradas).\n",
    "      * **Prueba:** Inicia sesión y verifica que puedes ver la lista de DAGs de ejemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4eacbb",
   "metadata": {},
   "source": [
    "#### 3\\. Probar la Conexión con Kafka\n",
    "\n",
    "Para validar que Kafka y Zookeeper funcionan y son accesibles desde tu entorno de Python (Jupyter), ejecuta el siguiente script en un notebook de Jupyter.\n",
    "\n",
    "  * **Prueba (Kafka):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e54bfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Productor Kafka conectado exitosamente.\n",
      "Mensaje enviado a 'test_topic': {'mensaje': 'Hola desde Jupyter!', 'id': 1}\n",
      "Consumidor Kafka conectado exitosamente.\n",
      "Mensaje recibido: {'mensaje': 'Hola desde Jupyter!', 'id': 1}\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "import json\n",
    "\n",
    "# Configuración\n",
    "KAFKA_TOPIC = 'test_topic'\n",
    "KAFKA_BOOTSTRAP_SERVERS = 'kafka:29092' # Usamos el nombre del servicio definido en docker-compose\n",
    "\n",
    "# 1. Crear Productor\n",
    "try:\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "        value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "    )\n",
    "    print(\"Productor Kafka conectado exitosamente.\")\n",
    "\n",
    "    # Enviar mensaje\n",
    "    message = {'mensaje': 'Hola desde Jupyter!', 'id': 1}\n",
    "    producer.send(KAFKA_TOPIC, message)\n",
    "    producer.flush()\n",
    "    print(f\"Mensaje enviado a '{KAFKA_TOPIC}': {message}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error conectando Productor: {e}\")\n",
    "\n",
    "# 2. Crear Consumidor\n",
    "try:\n",
    "    consumer = KafkaConsumer(\n",
    "        KAFKA_TOPIC,\n",
    "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "        auto_offset_reset='earliest', # Leer desde el principio\n",
    "        enable_auto_commit=True,\n",
    "        group_id='my-group',\n",
    "        value_deserializer=lambda x: json.loads(x.decode('utf-8')),\n",
    "        consumer_timeout_ms=5000 # Esperar 5 segundos y salir si no hay mensajes\n",
    "    )\n",
    "    print(\"Consumidor Kafka conectado exitosamente.\")\n",
    "\n",
    "    # Leer mensaje\n",
    "    for msg in consumer:\n",
    "        print(f\"Mensaje recibido: {msg.value}\")\n",
    "        break # Solo queremos probar que llega uno\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error conectando Consumidor: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312b437b",
   "metadata": {},
   "source": [
    "#### 4\\. Probar Spark + MLflow\n",
    "\n",
    "Verifica que Spark pueda procesar datos y registrar experimentos en MLflow. Puedes reusar o adaptar el código de tu `Cuaderno_1.ipynb`, asegurándote de que la URI de MLflow sea correcta.\n",
    "\n",
    "  * **Notebook de Prueba (Spark + MLflow):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0be3b203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.1\n",
      "Run registrado en MLflow\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import mlflow\n",
    "\n",
    "# 1. Inicializar Spark en modo LOCAL\n",
    "# Usamos \"local[*]\" para usar todos los núcleos del contenedor sin necesitar red\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test_Spark_MLflow\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "\n",
    "# 2. Configurar MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"Test_Experiment\")\n",
    "\n",
    "# 3. Registrar un run simple\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"param1\", 5)\n",
    "    mlflow.log_metric(\"metric1\", 0.85)\n",
    "    print(\"Run registrado en MLflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2f3bff",
   "metadata": {},
   "source": [
    "#### 5\\. Probar Airflow (Opcional)\n",
    "\n",
    "Para verificar que Airflow está escaneando correctamente tu carpeta `dags`, crea un archivo de prueba simple en tu carpeta local `dags/`.\n",
    "  * **Archivo `dags/test_dag.py`:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ccc043-b1ac-4ab3-97c9-06a741e67b02",
   "metadata": {},
   "source": [
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "with DAG(\n",
    "    'test_dag_simple',\n",
    "    start_date=datetime(2023, 1, 1),\n",
    "    schedule_interval=None,\n",
    "    catchup=False\n",
    ") as dag:\n",
    "\n",
    "    t1 = BashOperator(\n",
    "        task_id='print_date',\n",
    "        bash_command='date',\n",
    "    )\n",
    "\n",
    "    t2 = BashOperator(\n",
    "        task_id='echo_hello',\n",
    "        bash_command='echo \"Hola Airflow!\"',\n",
    "    )\n",
    "\n",
    "    t1 >> t2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38f6a4ed-cfcf-45b5-a560-f54931164c45",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'airflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mairflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DAG\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mairflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moperators\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbash\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BashOperator\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'airflow'"
     ]
    }
   ],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "with DAG(\n",
    "    'test_dag_simple',\n",
    "    start_date=datetime(2023, 1, 1),\n",
    "    schedule_interval=None,\n",
    "    catchup=False\n",
    ") as dag:\n",
    "\n",
    "    t1 = BashOperator(\n",
    "        task_id='print_date',\n",
    "        bash_command='date',\n",
    "    )\n",
    "\n",
    "    t2 = BashOperator(\n",
    "        task_id='echo_hello',\n",
    "        bash_command='echo \"Hola Airflow!\"',\n",
    "    )\n",
    "\n",
    "    t1 >> t2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4c4c11",
   "metadata": {},
   "source": [
    "  * **Validación:**\n",
    "\n",
    "    1.  Guarda el archivo.\n",
    "    2.  Espera unos segundos y refresca la UI de Airflow (`http://localhost:8081`).\n",
    "    3.  Deberías ver `test_dag_simple` en la lista.\n",
    "    4.  Actívalo (toggle \"On\") y ejecútalo manualmente (botón \"Play\").\n",
    "    5.  Verifica que las tareas se pongan en verde (Success).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e44146-11be-4b8f-ad1a-6a977728d5a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
