{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36f53b0a-18b8-4134-919a-f68a5cb55007",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/12 15:19:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/12 15:19:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/12/12 15:19:12 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de Spark: 3.5.1\n",
      "Versión de MLflow: 2.11.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/12 15:19:13 INFO mlflow.tracking.fluent: Experiment with name 'Experimento_Todo_En_Uno' does not exist. Creating a new experiment.\n",
      "2025/12/12 15:19:13 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh(<full-path-to-git-executable>)\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial message can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|silent|none|n|0: for no message or exception\n",
      "    - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)\n",
      "    - error|e|exception|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando un trabajo simple en Spark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|componente|valor|\n",
      "+----------+-----+\n",
      "|    Python|   10|\n",
      "|     Spark|   20|\n",
      "|    MLflow|   30|\n",
      "+----------+-----+\n",
      "\n",
      "Experimento y run de MLflow registrados con éxito.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1. Spark se conecta localmente\n",
    "# Usamos 'local[*]' para usar todos los cores disponibles en el contenedor\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Test Integrado\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"16g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(f\"Versión de Spark: {spark.version}\")\n",
    "print(f\"Versión de MLflow: {mlflow.__version__}\")\n",
    "\n",
    "# 2. MLflow se conecta a su servidor en localhost (dentro del contenedor)\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "# 3. Ejecutar un experimento de MLflow\n",
    "mlflow.set_experiment(\"Experimento_Todo_En_Uno\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"Run de Prueba\"):\n",
    "    mlflow.log_param(\"entorno\", \"Contenedor Único\")\n",
    "    \n",
    "    print(\"Ejecutando un trabajo simple en Spark...\")\n",
    "    data = [(\"Python\", 10), (\"Spark\", 20), (\"MLflow\", 30)]\n",
    "    df = spark.createDataFrame(data, [\"componente\", \"valor\"])\n",
    "    \n",
    "    df.show()\n",
    "    \n",
    "    avg_value = df.toPandas()['valor'].mean()\n",
    "    mlflow.log_metric(\"valor_promedio\", avg_value)\n",
    "    \n",
    "    print(\"Experimento y run de MLflow registrados con éxito.\")\n",
    "\n",
    "spark.stop()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4867bb-3e40-4c91-abb4-d81a8cd4e5fb",
   "metadata": {},
   "source": [
    "## Configuración del Entorno de Spark\n",
    "El primer paso consiste en inicializar una SparkSession, que es el punto de entrada para programar con la API de Spark. El siguiente script también automatiza la descarga del conjunto de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dd6e72b-4051-472b-b2af-de2563db90bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/12 15:19:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/12/12 15:19:22 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando descarga del conjunto de datos: yellow_tripdata_2024-01.parquet\n",
      "Descarga finalizada.\n",
      "SparkSession iniciada. Versión: 3.5.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://a04b70966fea:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TutorialComputacionDistribuida</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x71fb6c5c6d90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# 1. Inicialización de la SparkSession\n",
    "# El master 'local[*]' instruye a Spark para utilizar todos los núcleos de CPU\n",
    "# disponibles en la máquina local, simulando un entorno de trabajo paralelo.\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"TutorialComputacionDistribuida\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 2. Descarga y gestión del conjunto de datos\n",
    "data_dir = \"data\"\n",
    "filename = \"yellow_tripdata_2024-01.parquet\"\n",
    "filepath = os.path.join(data_dir, filename)\n",
    "\n",
    "if not os.path.exists(filepath):\n",
    "    print(f\"Iniciando descarga del conjunto de datos: {filename}\")\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{filename}\"\n",
    "    urllib.request.urlretrieve(url, filepath)\n",
    "    print(\"Descarga finalizada.\")\n",
    "else:\n",
    "    print(\"El conjunto de datos ya se encuentra en el directorio local.\")\n",
    "\n",
    "# 3. Verificación del contexto de Spark\n",
    "print(f\"SparkSession iniciada. Versión: {spark.version}\")\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d78d83-9675-48c0-9644-6ee34a2724ac",
   "metadata": {},
   "source": [
    "## Ejercicio 1: El Cuello de Botella del Mononodo con Pandas\n",
    "Este ejercicio demuestra las limitaciones inherentes al procesamiento en un solo nodo. Se utilizará la librería Pandas para cargar y procesar el dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75520ac9-c678-44d9-8731-ccfff24fb807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando el conjunto de datos en un DataFrame de Pandas...\n",
      "CPU times: user 666 ms, sys: 5.7 s, total: 6.37 s\n",
      "Wall time: 1.07 s\n",
      "CPU times: user 17.7 ms, sys: 3.2 ms, total: 20.9 ms\n",
      "Wall time: 19.1 ms\n",
      "\n",
      "--- Análisis de Rendimiento con Pandas ---\n",
      "Consumo de memoria RAM: 0.56 GB\n",
      "\n",
      "Resultado del conteo de pasajeros:\n",
      "passenger_count\n",
      "1.0    2188739\n",
      "2.0     405103\n",
      "3.0      91262\n",
      "4.0      51974\n",
      "5.0      33506\n",
      "0.0      31465\n",
      "6.0      22353\n",
      "8.0         51\n",
      "7.0          8\n",
      "9.0          1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"Cargando el conjunto de datos en un DataFrame de Pandas...\")\n",
    "\n",
    "# Medición del tiempo de carga y consumo de memoria\n",
    "%time df_pandas = pd.read_parquet(filepath)\n",
    "\n",
    "# Medición del tiempo de una operación de agregación simple\n",
    "%time conteo_pandas = df_pandas['passenger_count'].value_counts()\n",
    "\n",
    "print(\"\\n--- Análisis de Rendimiento con Pandas ---\")\n",
    "print(f\"Consumo de memoria RAM: {df_pandas.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
    "print(\"\\nResultado del conteo de pasajeros:\")\n",
    "print(conteo_pandas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423e9d9f-e79e-4050-8588-47bb3abc1a7f",
   "metadata": {},
   "source": [
    "## Ejercicio 2: Ingesta de Datos Distribuida y Evaluación Perezosa\n",
    "A continuación, se realiza la misma operación de carga utilizando Spark para ilustrar un concepto fundamental: la evaluación perezosa (lazy evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cb48a18-2292-44ac-b4fc-db8e36948f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando el conjunto de datos en un DataFrame de Spark...\n",
      "CPU times: user 7.37 ms, sys: 123 ms, total: 131 ms\n",
      "Wall time: 516 ms\n",
      "CPU times: user 3.85 ms, sys: 58.9 ms, total: 62.7 ms\n",
      "Wall time: 615 ms\n",
      "\n",
      "El DataFrame de Spark contiene 2964624 registros.\n"
     ]
    }
   ],
   "source": [
    "print(\"Cargando el conjunto de datos en un DataFrame de Spark...\")\n",
    "\n",
    "# La lectura del archivo es una transformación perezosa. Es instantánea.\n",
    "%time df_spark = spark.read.parquet(filepath)\n",
    "\n",
    "# .count() es una acción, la cual dispara la ejecución del trabajo.\n",
    "%time total_filas = df_spark.count()\n",
    "\n",
    "print(f\"\\nEl DataFrame de Spark contiene {total_filas} registros.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ef89e8-1051-4789-982d-595ae1be3f1d",
   "metadata": {},
   "source": [
    "## Ejercicio 3: Paralelismo y Particiones\n",
    "El paralelismo en Spark se logra dividiendo los datos en particiones. Un DataFrame es una abstracción sobre un RDD (Resilient Distributed Dataset), el cual es una colección de elementos particionados. Cada partición puede ser procesada de forma independiente y en paralelo por un executor en un nodo del clúster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c38d9deb-26d3-43d9-8f12-d3cad265bf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El DataFrame ha sido dividido en 12 particiones.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/12 15:19:34 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "25/12/12 15:19:34 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "[Stage 4:===============================================>         (10 + 2) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribución de registros en 12 particiones:\n",
      "[0, 1048576, 0, 0, 0, 1048576, 0, 0, 0, 867472, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 1. Obtener el número de particiones por defecto\n",
    "num_particiones = df_spark.rdd.getNumPartitions()\n",
    "print(f\"El DataFrame ha sido dividido en {num_particiones} particiones.\")\n",
    "\n",
    "# 2. Inspeccionar la distribución de registros por partición\n",
    "# Se utiliza el RDD subyacente para aplicar una función a cada partición.\n",
    "filas_por_particion = df_spark.rdd.mapPartitions(lambda iter: [sum(1 for _ in iter)]).collect()\n",
    "\n",
    "print(f\"\\nDistribución de registros en {len(filas_por_particion)} particiones:\")\n",
    "print(filas_por_particion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6bcadd-ff3d-4a74-bd68-7b6ea44fca5c",
   "metadata": {},
   "source": [
    "## Ejercicio 4: Modelo de Ejecución - Transformaciones y Acciones\n",
    "El modelo de programación de Spark se basa en la construcción de un plan de ejecución a través de transformaciones, que culmina con la ejecución de dicho plan mediante una acción.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d321950-c60a-4417-ba8e-4e59d1c909fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan de ejecución optimizado por Spark (DAG):\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [passenger_count#16L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(passenger_count#16L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=80]\n",
      "      +- HashAggregate(keys=[passenger_count#16L], functions=[avg(propina_pct#76), count(1)])\n",
      "         +- Exchange hashpartitioning(passenger_count#16L, 200), ENSURE_REQUIREMENTS, [plan_id=77]\n",
      "            +- HashAggregate(keys=[passenger_count#16L], functions=[partial_avg(propina_pct#76), partial_count(1)])\n",
      "               +- Project [passenger_count#16L, ((tip_amount#26 / total_amount#29) * 100.0) AS propina_pct#76]\n",
      "                  +- Filter (((isnotnull(passenger_count#16L) AND isnotnull(trip_distance#17)) AND (passenger_count#16L > 1)) AND (trip_distance#17 > 5.0))\n",
      "                     +- FileScan parquet [passenger_count#16L,trip_distance#17,tip_amount#26,total_amount#29] Batched: true, DataFilters: [isnotnull(passenger_count#16L), isnotnull(trip_distance#17), (passenger_count#16L > 1), (trip_di..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/app/notebooks/data/yellow_tripdata_2024-01.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(passenger_count), IsNotNull(trip_distance), GreaterThan(passenger_count,1), GreaterTha..., ReadSchema: struct<passenger_count:bigint,trip_distance:double,tip_amount:double,total_amount:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Definición del Plan Lógico (Transformaciones) ---\n",
    "# Estas operaciones son perezosas y construyen un Grafo Acíclico Dirigido (DAG).\n",
    "\n",
    "# Filtro de viajes con más de 1 pasajero y distancia superior a 5 millas\n",
    "df_filtrado = df_spark.filter(\n",
    "    (F.col(\"passenger_count\") > 1) &\n",
    "    (F.col(\"trip_distance\") > 5)\n",
    ")\n",
    "\n",
    "# Creación de una nueva columna con el porcentaje de propina\n",
    "df_con_propina_pct = df_filtrado.withColumn(\n",
    "    \"propina_pct\",\n",
    "    (F.col(\"tip_amount\") / F.col(\"total_amount\")) * 100\n",
    ")\n",
    "\n",
    "# Agregación para calcular la propina promedio por número de pasajeros\n",
    "df_agrupado = df_con_propina_pct.groupBy(\"passenger_count\").agg(\n",
    "    F.avg(\"propina_pct\").alias(\"propina_promedio_pct\"),\n",
    "    F.count(\"*\").alias(\"conteo_viajes\")\n",
    ")\n",
    "\n",
    "# Ordenamiento del resultado\n",
    "df_plan_final = df_agrupado.orderBy(F.col(\"passenger_count\").desc())\n",
    "\n",
    "# --- 2. Inspección del Plan Físico de Ejecución ---\n",
    "print(\"Plan de ejecución optimizado por Spark (DAG):\")\n",
    "df_plan_final.explain()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f5a02d-eae4-4b96-aee6-f75b6313e4cb",
   "metadata": {},
   "source": [
    "## Ejercicio 5: Ejecución del Plan y la Operación de Shuffle\n",
    "Finalmente, una acción como .collect() o .show() desencadena la ejecución del DAG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "185de3f6-d3de-4233-9198-0954a912852a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ejecutando el plan de computación distribuida... ---\n",
      "CPU times: user 1.16 ms, sys: 7.93 ms, total: 9.09 ms\n",
      "Wall time: 1.06 s\n",
      "\n",
      "--- Resultados del Análisis ---\n",
      "Pasajeros: 8, \tConteo: 5, \tPropina Promedio: 11.98%\n",
      "Pasajeros: 7, \tConteo: 2, \tPropina Promedio: 18.32%\n",
      "Pasajeros: 6, \tConteo: 2917, \tPropina Promedio: 11.75%\n",
      "Pasajeros: 5, \tConteo: 4624, \tPropina Promedio: 11.64%\n",
      "Pasajeros: 4, \tConteo: 10183, \tPropina Promedio: 9.57%\n",
      "Pasajeros: 3, \tConteo: 16344, \tPropina Promedio: 10.46%\n",
      "Pasajeros: 2, \tConteo: 74759, \tPropina Promedio: 11.18%\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Ejecución del Plan (Acción) ---\n",
    "# La acción .collect() transfiere los resultados al nodo driver.\n",
    "# ADVERTENCIA: Usar .collect() solo con resultados de tamaño manejable.\n",
    "print(\"\\n--- Ejecutando el plan de computación distribuida... ---\")\n",
    "\n",
    "%time resultados = df_plan_final.collect()\n",
    "\n",
    "print(\"\\n--- Resultados del Análisis ---\")\n",
    "for fila in resultados:\n",
    "    print(f\"Pasajeros: {fila['passenger_count']}, \\tConteo: {fila['conteo_viajes']}, \\tPropina Promedio: {fila['propina_promedio_pct']:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b178b0c2-bb71-4db4-a546-26609b722084",
   "metadata": {},
   "source": [
    "## Clase 2\n",
    "\n",
    "Continuamos estudiando Spark. Observa atentamente los tiempos de ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44fd0d2f-49ca-47ed-a8f2-512a32e68c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando el DataFrame en Spark...\n",
      "data/yellow_tripdata_2024-01.parquet\n",
      "CPU times: user 959 μs, sys: 2.82 ms, total: 3.78 ms\n",
      "Wall time: 109 ms\n",
      "\n",
      "--- Ejecutando una 'Acción' (como .count()) ---\n",
      "CPU times: user 314 μs, sys: 925 μs, total: 1.24 ms\n",
      "Wall time: 240 ms\n",
      "El DataFrame de Spark contiene 2964624 registros.\n"
     ]
    }
   ],
   "source": [
    "print(\"Cargando el DataFrame en Spark...\")\n",
    "print(filepath)\n",
    "%time df_spark = spark.read.parquet(filepath)\n",
    "\n",
    "print(\"\\n--- Ejecutando una 'Acción' (como .count()) ---\")\n",
    "%time total_filas = df_spark.count()\n",
    "\n",
    "print(f\"El DataFrame de Spark contiene {total_filas} registros.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd806eb1-1460-4054-9654-9c2bfa96e03b",
   "metadata": {},
   "source": [
    "**Análisis y Filosofía de Spark:**\n",
    "\n",
    "* `spark.read.parquet(filepath)`: ¡El tiempo de ejecución fue casi **cero** (milisegundos)!\n",
    "* **¿Por Qué?** Porque Spark no cargó los datos. Esto es la **Evaluación Perezosa (Lazy Evaluation)**.\n",
    "* **Transformaciones (La Receta):** `spark.read.parquet` es una **Transformación**. Es una instrucción, una \"promesa\" de que *eventualmente* leerás ese archivo. Spark simplemente anota en su plan: \"OK, empezaré por este archivo\".\n",
    "* **Acciones (La Orden):** `.count()` es una **Acción**. Es la orden de \"¡Ejecuta el plan y dame un resultado!\". Solo cuando pides una acción, Spark realmente empieza a trabajar.\n",
    "* **La Ventaja:** Esto permite a Spark construir un plan de ejecución complejo y optimizarlo *antes* de mover un solo byte de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e303e52-0fe9-46fb-8c89-62a27f0f0d9a",
   "metadata": {},
   "source": [
    "## Ejercicio ¿Cómo procesa Spark?\n",
    "¿Cómo procesa Spark los datos sin cargarlos todos a la RAM? Dividiéndolos en **Particiones**. Una partición es un \"trozo\" del conjunto de datos que puede ser procesado por un trabajador (un núcleo de CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fa56039-68f0-4ee7-9508-bfe17724461f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El DataFrame ha sido dividido en 12 particiones.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:==============================================>         (10 + 2) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribución de registros en 12 particiones:\n",
      "[0, 1048576, 0, 0, 0, 1048576, 0, 0, 0, 867472, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 1. ¿Cuántas particiones (lotes) creó Spark?\n",
    "num_particiones = df_spark.rdd.getNumPartitions()\n",
    "print(f\"El DataFrame ha sido dividido en {num_particiones} particiones.\")\n",
    "\n",
    "# 2. Miremos DENTRO de las particiones (¿cuántas filas tiene cada una?)\n",
    "filas_por_particion = df_spark.rdd.mapPartitions(lambda iter: [sum(1 for _ in iter)]).collect()\n",
    "print(f\"\\nDistribución de registros en {len(filas_por_particion)} particiones:\")\n",
    "print(filas_por_particion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1749f029-c87d-4eee-88e4-bddac4b2890c",
   "metadata": {},
   "source": [
    "**Salida Esperada (¡Nuestra Primera Pista!):**\n",
    "\n",
    "El DataFrame ha sido dividido en 12 particiones. Distribución de registros en 12 particiones: [0, 0, 1048576, 0, 0, 0, 1048576, 0, 0, 0, 867472, 0]\n",
    "\n",
    "\n",
    "**Análisis y Filosofía de Spark:**\n",
    "\n",
    "* `df_spark.rdd.getNumPartitions()`: `rdd` es la API de bajo nivel de Spark. Le estamos preguntando: \"¿En cuántos lotes se dividió el trabajo?\".\n",
    "* **Grado de Paralelismo:** Responde 12. Esto es porque nuestro `.master(\"local[*]\")` detectó 12 núcleos de CPU, por lo que 12 es el número de hilos \"trabajadores\" por defecto.\n",
    "* **Desbalanceo de Datos (Data Skew):** ¡Este es el punto clave! La lista `[0, 0, 1048576, ...]` nos dice que **9 de nuestros 12 trabajadores no están haciendo nada**. Todo el trabajo de lectura se concentra en solo 3 particiones (trabajadores).\n",
    "* **¿Por qué?** No es un error. Es un artefacto de la lectura. El archivo Parquet que descargamos está almacenado en el disco en 3 \"Row Groups\" (bloques) principales. Spark, al leerlo, asignó cada bloque a una partición.\n",
    "* **Conclusión:** Aunque tenemos 12 núcleos listos, en la primera etapa del trabajo, solo 3 están activos. **Lo arreglaremos en lo que sigue.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d21c396-9a55-4730-9b63-f4d7a73b888d",
   "metadata": {},
   "source": [
    "**Análisis y Filosofía de Spark:**\n",
    "\n",
    "* `df_spark.rdd.getNumPartitions()`: `rdd` es la API de bajo nivel de Spark. Le estamos preguntando: \"¿En cuántos lotes se dividió el trabajo?\".\n",
    "* **Grado de Paralelismo:** Responde 12. Esto es porque nuestro `.master(\"local[*]\")` detectó 12 núcleos de CPU, por lo que 12 es el número de hilos \"trabajadores\" por defecto.\n",
    "* **Desbalanceo de Datos (Data Skew):** ¡Este es el punto clave! La lista `[0, 0, 1048576, ...]` nos dice que **9 de nuestros 12 trabajadores no están haciendo nada**. Todo el trabajo de lectura se concentra en solo 3 particiones (trabajadores).\n",
    "* **¿Por qué?** No es un error. Es un artefacto de la lectura. El archivo Parquet que descargamos está almacenado en el disco en 3 \"Row Groups\" (bloques) principales. Spark, al leerlo, asignó cada bloque a una partición.\n",
    "* **Conclusión:** Aunque tenemos 12 núcleos listos, en la primera etapa del trabajo, solo 3 están activos. **Lo arreglaremos en el Ejercicio 6.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15badb0b-3876-4bce-a431-49517122f6bd",
   "metadata": {},
   "source": [
    "## Ejercicio: Optimizando - Uso Completo de Recursos\n",
    "\n",
    "Solo 3 de nuestros 12 núcleos estaban trabajando en la lectura. Vamos a arreglar esto.\n",
    "\n",
    "**La Solución:** `repartition()`\n",
    "Le daremos a Spark una orden explícita para que baraje los datos *inmediatamente* después de leerlos y los redistribuya equitativamente en 12 particiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49e92ad9-2c62-404a-b726-a9aa567ef9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Particiones originales: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:=======================>                                 (5 + 7) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuevas particiones: 12\n",
      "\n",
      "Verificando la nueva distribución (esto tomará un momento)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:============================>                            (6 + 6) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de registros en 12 particiones (después de repartition):\n",
      "[247052, 247051, 247052, 247052, 247053, 247053, 247052, 247052, 247051, 247052, 247052, 247052]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: int, tpep_pickup_datetime: timestamp_ntz, tpep_dropoff_datetime: timestamp_ntz, passenger_count: bigint, trip_distance: double, RatecodeID: bigint, store_and_fwd_flag: string, PULocationID: int, DOLocationID: int, payment_type: bigint, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, improvement_surcharge: double, total_amount: double, congestion_surcharge: double, Airport_fee: double]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Creamos un nuevo DataFrame, forzando la redistribución\n",
    "print(f\"Particiones originales: {df_spark.rdd.getNumPartitions()}\")\n",
    "df_reparticionado = df_spark.repartition(12)\n",
    "\n",
    "# 2. Verifiquemos la nueva distribución\n",
    "# .cache() es una transformación que le dice a Spark \"guarda el resultado de\n",
    "# esta repartición en memoria para que no tengamos que hacerla de nuevo\".\n",
    "df_reparticionado.cache()\n",
    "print(f\"Nuevas particiones: {df_reparticionado.rdd.getNumPartitions()}\")\n",
    "print(\"\\nVerificando la nueva distribución (esto tomará un momento)...\")\n",
    "\n",
    "# Esta acción (.count()) fuerza la ejecución del .repartition() y .cache()\n",
    "df_reparticionado.count() \n",
    "\n",
    "# Ahora miremos dentro de las particiones cacheadas\n",
    "nuevas_filas_por_particion = df_reparticionado.rdd.mapPartitions(lambda iter: [sum(1 for _ in iter)]).collect()\n",
    "print(f\"Distribución de registros en 12 particiones (después de repartition):\")\n",
    "print(nuevas_filas_por_particion)\n",
    "\n",
    "# Liberar la memoria cacheada\n",
    "df_reparticionado.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc90c81-f942-4776-8922-a13cca5b9b0c",
   "metadata": {},
   "source": [
    "**Análisis y Filosofía de Spark:**\n",
    "\n",
    "* `df_spark.repartition(12)`: Es una **Transformación Ancha (Wide)**. Le dimos a Spark la orden explícita de \"ejecutar un SHUFFLE completo y redistribuir todos los datos en 12 nuevas particiones de tamaño (casi) idéntico\".\n",
    "* **¡Éxito!** La nueva distribución de filas es perfectamente balanceada.\n",
    "* **La Ventaja:** Ahora, cualquier operación que hagamos (como nuestro `filter` o `project`) utilizará **los 12 núcleos en paralelo** desde el primer segundo.\n",
    "* **El Costo:** `repartition()` es en sí mismo un *shuffle* completo, ¡lo cual es costoso! Introdujimos un costo inicial, pero lo hicimos para que el resto del procesamiento sea mucho más rápido y eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b14e51-5b46-4015-9a30-8195917e5898",
   "metadata": {},
   "source": [
    "## Sintáxis de spark\n",
    "\n",
    "### Parte 1: Selección y Filtro (El `SELECT` y `WHERE` de SQL)\n",
    "\n",
    "Empecemos por lo básico: cómo seleccionar columnas y filtrar filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4aeac506-8d14-40d2-bc4a-54c9aed0aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siempre importamos las funciones de SQL\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Carguemos nuestro DataFrame original (el desbalanceado está bien para esto)\n",
    "df_spark = spark.read.parquet(\"data/yellow_tripdata_2024-01.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ecb6c0-5c89-4e61-9a0e-2d1325a3ddec",
   "metadata": {},
   "source": [
    "La función más básica. Le dices qué columnas quieres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "380ff47b-fdc1-4b1e-a2f4-1443dfd8e72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame solo con 3 columnas:\n",
      "+---------------+-------------+------------+\n",
      "|passenger_count|trip_distance|total_amount|\n",
      "+---------------+-------------+------------+\n",
      "|              1|         1.72|        22.7|\n",
      "|              1|          1.8|       18.75|\n",
      "|              1|          4.7|        31.3|\n",
      "|              1|          1.4|        17.0|\n",
      "|              1|          0.8|        16.1|\n",
      "+---------------+-------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL: SELECT passenger_count, trip_distance FROM ...\n",
    "df_seleccion = df_spark.select(\"passenger_count\", \"trip_distance\", \"total_amount\")\n",
    "\n",
    "print(\"DataFrame solo con 3 columnas:\")\n",
    "df_seleccion.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41c5968-1e71-4dfc-8c8a-824472061064",
   "metadata": {},
   "source": [
    "`selectExpr` (Select Expression) te permite escribir pseudo-SQL dentro de una string de Python. Es muy útil para cálculos rápidos o para renombrar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c6ddf0d-a32c-48ba-8575-7fc8e7604892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame con columnas calculadas y renombradas:\n",
      "+-------------+------------------+---------+\n",
      "|trip_distance|  trip_distance_km|pasajeros|\n",
      "+-------------+------------------+---------+\n",
      "|         1.72|         2.7680648|        1|\n",
      "|          1.8|          2.896812|        1|\n",
      "|          4.7|          7.563898|        1|\n",
      "|          1.4|2.2530759999999996|        1|\n",
      "|          0.8|1.2874720000000002|        1|\n",
      "+-------------+------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL: SELECT trip_distance, trip_distance * 1.60934 AS trip_distance_km FROM ...\n",
    "df_expr = df_spark.selectExpr(\n",
    "    \"trip_distance\", \n",
    "    \"trip_distance * 1.60934 AS trip_distance_km\",\n",
    "    \"passenger_count AS pasajeros\"\n",
    ")\n",
    "\n",
    "print(\"DataFrame con columnas calculadas y renombradas:\")\n",
    "df_expr.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1698addc-559c-4dac-9af5-6c2d6c7fb7e1",
   "metadata": {},
   "source": [
    "**`filter()` y `F.col()` - El `WHERE` de SQL**\n",
    "\n",
    "Aquí es donde entra la filosofía de `F.col()`.\n",
    "\n",
    "* `\"passenger_count\"` (string): Es solo el *nombre* de la columna.\n",
    "* `F.col(\"passenger_count\")` (objeto Columna): Es la *columna real* sobre la que podemos hacer operaciones (como `>`, `==`, `+`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71b47bd5-d891-4ed2-9f9f-ee2f7363304a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viajes largos con más de 1 pasajero:\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2024-01-01 00:49:44|  2024-01-01 01:15:47|              2|        10.82|         1|                 N|         138|         181|           1|       45.7|  6.0|    0.5|      10.0|         0.0|                  1.0|       64.95|                 0.0|       1.75|\n",
      "|       1| 2024-01-01 00:35:16|  2024-01-01 01:11:52|              2|          8.2|         1|                 N|         246|         190|           1|       59.0|  3.5|    0.5|     14.15|        6.94|                  1.0|       85.09|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:49:31|  2024-01-01 01:35:41|              2|         8.89|         1|                 N|          79|          41|           1|       47.8|  1.0|    0.5|      7.92|         0.0|                  1.0|       60.72|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:40:01|  2024-01-01 01:18:26|              2|         5.27|         1|                 N|         232|         246|           1|       38.0|  1.0|    0.5|       8.6|         0.0|                  1.0|        51.6|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:45:26|  2024-01-01 01:23:16|              2|         5.35|         1|                 N|          79|          48|           1|       34.5|  1.0|    0.5|      6.08|         0.0|                  1.0|       45.58|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL: WHERE passenger_count > 1 AND trip_distance > 5\n",
    "df_filtrado = df_spark.filter(\n",
    "    (F.col(\"passenger_count\") > 1) & \n",
    "    (F.col(\"trip_distance\") > 5)\n",
    ")\n",
    "\n",
    "print(\"Viajes largos con más de 1 pasajero:\")\n",
    "df_filtrado.show(5)\n",
    "\n",
    "# Recordatorio de sintaxis:\n",
    "# En PySpark, usa '&' (AND), '|' (OR), y '~' (NOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7629325d-52b7-4637-9682-e69f8deebc73",
   "metadata": {},
   "source": [
    "**`where()` - El alias de `filter()`**\n",
    "\n",
    "`where()` hace *exactamente* lo mismo que `filter()`. Es solo un alias para que los que vienen de SQL se sientan más cómodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae297b21-0122-4f47-bfb5-a0de1f9a8741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viajes pagados con Tarjeta de Crédito (payment_type 1):\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       1| 2024-01-01 00:03:00|  2024-01-01 00:09:36|              1|          1.8|         1|                 N|         140|         236|           1|       10.0|  3.5|    0.5|      3.75|         0.0|                  1.0|       18.75|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:17:06|  2024-01-01 00:35:01|              1|          4.7|         1|                 N|         236|          79|           1|       23.3|  3.5|    0.5|       3.0|         0.0|                  1.0|        31.3|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:36:38|  2024-01-01 00:44:56|              1|          1.4|         1|                 N|          79|         211|           1|       10.0|  3.5|    0.5|       2.0|         0.0|                  1.0|        17.0|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:46:51|  2024-01-01 00:52:57|              1|          0.8|         1|                 N|         211|         148|           1|        7.9|  3.5|    0.5|       3.2|         0.0|                  1.0|        16.1|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:54:08|  2024-01-01 01:26:31|              1|          4.7|         1|                 N|         148|         141|           1|       29.6|  3.5|    0.5|       6.9|         0.0|                  1.0|        41.5|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL: WHERE payment_type = 1\n",
    "df_where = df_spark.where(F.col(\"payment_type\") == 1)\n",
    "\n",
    "print(\"Viajes pagados con Tarjeta de Crédito (payment_type 1):\")\n",
    "df_where.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d85b9fc-2011-4bb8-afe5-b4217c01a1fc",
   "metadata": {},
   "source": [
    "**`like()` - Búsqueda de patrones**\n",
    "\n",
    "Al igual que en SQL, `like()` busca patrones en strings. `store_and_fwd_flag` es una columna con 'Y' o 'N'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ff0c154-6cb3-435c-980d-cb268f2bd8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viajes con 'store_and_fwd_flag' = Y:\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       1| 2024-01-01 00:51:58|  2024-01-01 01:05:44|              2|          2.9|         1|                 Y|         264|         264|           1|       16.3|  3.5|    0.5|      4.25|         0.0|                  1.0|       25.55|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:35:56|  2024-01-01 00:56:40|              2|          2.6|         1|                 Y|         239|         162|           2|       17.0|  3.5|    0.5|       0.0|         0.0|                  1.0|        22.0|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:54:29|  2024-01-01 01:24:52|              1|          2.6|         1|                 Y|          50|         170|           2|       25.4|  3.5|    0.5|       0.0|         0.0|                  1.0|        30.4|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:11:26|  2024-01-01 00:32:29|              1|         2.21|         1|                 Y|         158|         186|           1|       19.8|  1.0|    0.5|       1.0|         0.0|                  1.0|        25.8|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:27:01|  2024-01-01 00:38:01|              1|          1.5|         1|                 Y|         234|         246|           1|       12.1|  3.5|    0.5|       5.1|         0.0|                  1.0|        22.2|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:08:07|  2024-01-01 00:14:19|              1|          1.1|         1|                 Y|         141|         262|           1|        8.6|  3.5|    0.5|       3.4|         0.0|                  1.0|        17.0|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:53:06|  2024-01-01 01:26:22|              1|          5.0|         1|                 Y|         263|         249|           1|       32.4|  3.5|    0.5|      7.45|         0.0|                  1.0|       44.85|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:04:52|  2024-01-01 00:30:01|              1|          3.1|         1|                 Y|          48|         142|           1|       24.0|  3.5|    0.5|       5.8|         0.0|                  1.0|        34.8|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:34:26|  2024-01-01 00:46:10|              1|          1.8|         1|                 Y|         142|         140|           1|       13.5|  3.5|    0.5|       5.5|         0.0|                  1.0|        24.0|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:51:34|  2024-01-01 00:56:01|              1|          0.5|         1|                 Y|         141|         236|           4|        5.8|  3.5|    0.5|       0.0|         0.0|                  1.0|        10.8|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL: WHERE store_and_fwd_flag LIKE 'Y'\n",
    "df_like = df_spark.filter( F.col(\"store_and_fwd_flag\").like(\"Y\") )\n",
    "\n",
    "print(f\"Viajes con 'store_and_fwd_flag' = Y:\")\n",
    "df_like.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ca706e-713c-4328-91e3-08656e511d43",
   "metadata": {},
   "source": [
    "**`isin()` - Múltiples `OR`**\n",
    "\n",
    "`isin()` es un atajo para `WHERE mi_columna = 'A' OR mi_columna = 'B' ...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ab7282d-b1aa-41be-a92e-fefbb179f58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viajes pagados con Tarjeta o Efectivo:\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2024-01-01 00:57:55|  2024-01-01 01:17:43|              1|         1.72|         1|                 N|         186|          79|           2|       17.7|  1.0|    0.5|       0.0|         0.0|                  1.0|        22.7|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:03:00|  2024-01-01 00:09:36|              1|          1.8|         1|                 N|         140|         236|           1|       10.0|  3.5|    0.5|      3.75|         0.0|                  1.0|       18.75|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:17:06|  2024-01-01 00:35:01|              1|          4.7|         1|                 N|         236|          79|           1|       23.3|  3.5|    0.5|       3.0|         0.0|                  1.0|        31.3|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:36:38|  2024-01-01 00:44:56|              1|          1.4|         1|                 N|          79|         211|           1|       10.0|  3.5|    0.5|       2.0|         0.0|                  1.0|        17.0|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:46:51|  2024-01-01 00:52:57|              1|          0.8|         1|                 N|         211|         148|           1|        7.9|  3.5|    0.5|       3.2|         0.0|                  1.0|        16.1|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Busquemos viajes pagados con \"Tarjeta\" (1) o \"Efectivo\" (2)\n",
    "df_isin = df_spark.filter( F.col(\"payment_type\").isin(1, 2) )\n",
    "\n",
    "print(\"Viajes pagados con Tarjeta o Efectivo:\")\n",
    "df_isin.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553bcccc-c568-4571-af26-43cf9eefcf5d",
   "metadata": {},
   "source": [
    "### Parte 2: Manipulación de Columnas (Ingeniería de Características)\n",
    "\n",
    "Aquí es donde Spark brilla. Crear nuevas columnas es la base de la ingeniería de características (feature engineering)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89f7d73-577d-4a99-8e0c-1c50dbac6d4a",
   "metadata": {},
   "source": [
    "**`withColumn()` - La función más importante**\n",
    "\n",
    "`withColumn(\"nombre_columna\", <operación>)` es la forma estándar de crear una nueva columna o reemplazar una existente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a731775-59e4-48cf-b5ce-a9a1dd600e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame con 'propina_pct' (puede ser NaN si total_amount es 0):\n",
      "+--------+----------+------------+------------------+---------------+\n",
      "|VendorID|tip_amount|total_amount|       propina_pct|passenger_count|\n",
      "+--------+----------+------------+------------------+---------------+\n",
      "|       2|       0.0|        22.7|               0.0|              1|\n",
      "|       1|      3.75|       18.75|              20.0|              1|\n",
      "|       1|       3.0|        31.3| 9.584664536741213|              1|\n",
      "|       1|       2.0|        17.0| 11.76470588235294|              1|\n",
      "|       1|       3.2|        16.1|19.875776397515526|              1|\n",
      "+--------+----------+------------+------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos a crear la columna 'propina_pct' que usamos en la clase anterior\n",
    "df_con_columna = df_spark.withColumn(\n",
    "    \"propina_pct\",\n",
    "    (F.col(\"tip_amount\") / F.col(\"total_amount\")) * 100\n",
    ")\n",
    "\n",
    "print(\"DataFrame con 'propina_pct' (puede ser NaN si total_amount es 0):\")\n",
    "df_con_columna.select(\"VendorID\",\"tip_amount\", \"total_amount\", \"propina_pct\",\"passenger_count\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ecfa1b-033a-4294-801f-33978cb0e6a7",
   "metadata": {},
   "source": [
    "**`F.lit()` - Introduciendo un Valor Literal**\n",
    "\n",
    "¿Qué pasa si quieres añadir una columna donde *todas* las filas tengan el mismo valor (ej. \"Hola\", o el número 100)?\n",
    "\n",
    "No puedes solo escribir `\"Hola\"`, porque Spark pensará que es el *nombre* de una columna. Debes usar `F.lit()` (literal) para decirle \"este es un valor constante, no el nombre de una columna\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85d0f331-8676-475b-a90b-0856a9690856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|passenger_count|  fuente|\n",
      "+---------------+--------+\n",
      "|              1|taxis_ny|\n",
      "|              1|taxis_ny|\n",
      "|              1|taxis_ny|\n",
      "|              1|taxis_ny|\n",
      "|              1|taxis_ny|\n",
      "+---------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Añadimos una columna 'fuente' con el valor constante 'taxis_ny'\n",
    "df_con_literal = df_spark.withColumn(\"fuente\", F.lit(\"taxis_ny\"))\n",
    "\n",
    "df_con_literal.select(\"passenger_count\", \"fuente\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcf41f0-6654-4c6f-882c-f178425982f4",
   "metadata": {},
   "source": [
    "**`withColumnRenamed()` y `drop()` - Limpieza**\n",
    "\n",
    "Funciones básicas de limpieza: renombrar y eliminar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6033e8d5-2691-4e35-abc1-e2080408996a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "615020cb-b63c-4df9-9d46-c78340a4decb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas después de renombrar y eliminar:\n",
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- pasajeros: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renombremos 'passenger_count' a 'pasajeros' y eliminemos 'store_and_fwd_flag'\n",
    "df_limpio = df_spark.withColumnRenamed(\"passenger_count\", \"pasajeros\") \\\n",
    "                    .drop(\"store_and_fwd_flag\")\n",
    "\n",
    "print(\"Columnas después de renombrar y eliminar:\")\n",
    "df_limpio.printSchema() # .printSchema() nos muestra la estructura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c666e4-df31-46b4-a161-fffff396a9db",
   "metadata": {},
   "source": [
    "### Parte 3: El Poder de `F` (Lógica, Fechas y Nulos)\n",
    "\n",
    "Aquí es donde `pyspark.sql.functions` (nuestro `F`) realmente muestra su poder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d9b10c-50d5-4e92-980d-cecc3315c0cf",
   "metadata": {},
   "source": [
    "**`F.when()` - El `CASE WHEN` de SQL**\n",
    "\n",
    "Esta es *extremadamente* poderosa. Te permite crear lógica condicional (if/then/else) para una nueva columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c154e68f-4a0a-4c98-a5b2-346bce2027b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame con columna condicional 'tipo_viaje':\n",
      "+-------------+---------------+\n",
      "|trip_distance|     tipo_viaje|\n",
      "+-------------+---------------+\n",
      "|         1.72|          Corto|\n",
      "|          1.8|          Corto|\n",
      "|          4.7|          Corto|\n",
      "|          1.4|          Corto|\n",
      "|          0.8|          Corto|\n",
      "|          4.7|          Corto|\n",
      "|        10.82|Largo (No Aero)|\n",
      "|          3.0|          Corto|\n",
      "|         5.44|          Medio|\n",
      "|         0.04|          Corto|\n",
      "|         0.75|          Corto|\n",
      "|          1.2|          Corto|\n",
      "|          8.2|          Medio|\n",
      "|          0.4|          Corto|\n",
      "|          0.8|          Corto|\n",
      "|          5.0|          Corto|\n",
      "|          1.5|          Corto|\n",
      "|          0.0|          Corto|\n",
      "|          1.5|          Corto|\n",
      "|         2.57|          Corto|\n",
      "+-------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creemos una columna \"tipo_viaje\" basada en la distancia\n",
    "df_con_when = df_spark.withColumn(\"tipo_viaje\",\n",
    "    F.when(F.col(\"trip_distance\") > 20, F.lit(\"Largo (Aero)\"))\n",
    "        .when(F.col(\"trip_distance\") > 10, F.lit(\"Largo (No Aero)\"))\n",
    "     .when(F.col(\"trip_distance\") > 5, F.lit(\"Medio\"))\n",
    "     .otherwise(F.lit(\"Corto\"))\n",
    ")\n",
    "\n",
    "print(\"DataFrame con columna condicional 'tipo_viaje':\")\n",
    "df_con_when.select(\"trip_distance\", \"tipo_viaje\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30f1608-0e53-4164-a871-65b6f1b193f6",
   "metadata": {},
   "source": [
    "**Manejo de Nulos ( `isNull`, `isNotNull`, `fillna` )**\n",
    "\n",
    "Los datos del mundo real están sucios. `passenger_count` puede tener nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2c57543-84a1-42ce-aad5-e414dd91804f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viajes con 'passenger_count' nulo: 140162\n"
     ]
    }
   ],
   "source": [
    "# Contemos cuántos nulos hay\n",
    "conteo_nulos = df_spark.filter(F.col(\"passenger_count\").isNull()).count()\n",
    "print(f\"Viajes con 'passenger_count' nulo: {conteo_nulos}\")\n",
    "\n",
    "# Usemos fillna() para reemplazar nulos con un valor (ej. 1 pasajero por defecto)\n",
    "# .fillna() es un método del DataFrame, no de F.\n",
    "df_sin_nulos = df_spark.fillna(1, subset=[\"passenger_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79f93b66-9608-4350-a2c2-d01d7783085a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viajes con 'passenger_count' nulo en df_sin_nulos: 0\n"
     ]
    }
   ],
   "source": [
    "df_sin_nulos = df_spark.fillna(1, subset=[\"passenger_count\"])\n",
    "conteo_nulos = df_sin_nulos.filter(F.col(\"passenger_count\").isNull()).count()\n",
    "print(f\"Viajes con 'passenger_count' nulo en df_sin_nulos: {conteo_nulos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a23f25-ac74-469c-9a73-717d1a9d4891",
   "metadata": {},
   "source": [
    "**Funciones de Fecha/Hora (`year`, `month`, `dayofweek`)**\n",
    "\n",
    "Nuestras columnas `tpep_pickup_datetime` son *timestamps*. Podemos extraer sus componentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a39ab15a-9240-4f09-8b21-cbea2a89523e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame con componentes de fecha/hora extraídos:\n",
      "+--------------------+----+---+----------+----+\n",
      "|tpep_pickup_datetime| año|mes|dia_semana|hora|\n",
      "+--------------------+----+---+----------+----+\n",
      "| 2024-01-01 00:57:55|2024|  1|         2|   0|\n",
      "| 2024-01-01 00:03:00|2024|  1|         2|   0|\n",
      "| 2024-01-01 00:17:06|2024|  1|         2|   0|\n",
      "| 2024-01-01 00:36:38|2024|  1|         2|   0|\n",
      "| 2024-01-01 00:46:51|2024|  1|         2|   0|\n",
      "+--------------------+----+---+----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extraigamos el año, mes, día y hora de recogida\n",
    "df_con_fechas = df_spark.withColumn(\"año\", F.year(F.col(\"tpep_pickup_datetime\"))) \\\n",
    "                        .withColumn(\"mes\", F.month(F.col(\"tpep_pickup_datetime\"))) \\\n",
    "                        .withColumn(\"dia_semana\", F.dayofweek(F.col(\"tpep_pickup_datetime\"))) \\\n",
    "                        .withColumn(\"hora\", F.hour(F.col(\"tpep_pickup_datetime\")))\n",
    "\n",
    "print(\"DataFrame con componentes de fecha/hora extraídos:\")\n",
    "df_con_fechas.select(\"tpep_pickup_datetime\", \"año\", \"mes\", \"dia_semana\", \"hora\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85568ca0-0847-4cff-85b0-8f1a0555991f",
   "metadata": {},
   "source": [
    "**Cálculo de Duración**\n",
    "\n",
    "Podemos hacer aritmética con fechas. Calculemos la duración del viaje en segundos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fccab3bc-a57d-45e7-beb9-036e156cdf24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[DATATYPE_MISMATCH.CAST_WITHOUT_SUGGESTION] Cannot resolve \"CAST(tpep_dropoff_datetime AS BIGINT)\" due to data type mismatch: cannot cast \"TIMESTAMP_NTZ\" to \"BIGINT\".;\n'Project [VendorID#1174, tpep_pickup_datetime#1175, tpep_dropoff_datetime#1176, passenger_count#1177L, trip_distance#1178, RatecodeID#1179L, store_and_fwd_flag#1180, PULocationID#1181, DOLocationID#1182, payment_type#1183L, fare_amount#1184, extra#1185, mta_tax#1186, tip_amount#1187, tolls_amount#1188, improvement_surcharge#1189, total_amount#1190, congestion_surcharge#1191, Airport_fee#1192, (cast(tpep_dropoff_datetime#1176 as bigint) - cast(tpep_pickup_datetime#1175 as bigint)) AS duracion_segundos#1946]\n+- Relation [VendorID#1174,tpep_pickup_datetime#1175,tpep_dropoff_datetime#1176,passenger_count#1177L,trip_distance#1178,RatecodeID#1179L,store_and_fwd_flag#1180,PULocationID#1181,DOLocationID#1182,payment_type#1183L,fare_amount#1184,extra#1185,mta_tax#1186,tip_amount#1187,tolls_amount#1188,improvement_surcharge#1189,total_amount#1190,congestion_surcharge#1191,Airport_fee#1192] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Restar timestamps nos da un \"Intervalo\". \u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Necesitamos convertirlo a segundos usando .cast(\"long\")\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df_con_duracion = \u001b[43mdf_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mduracion_segundos\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtpep_dropoff_datetime\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlong\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtpep_pickup_datetime\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlong\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m df_con_duracion.select(\u001b[33m\"\u001b[39m\u001b[33mduracion_segundos\u001b[39m\u001b[33m\"\u001b[39m).show(\u001b[32m5\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/sql/dataframe.py:5174\u001b[39m, in \u001b[36mDataFrame.withColumn\u001b[39m\u001b[34m(self, colName, col)\u001b[39m\n\u001b[32m   5169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[32m   5170\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m   5171\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mNOT_COLUMN\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5172\u001b[39m         message_parameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcol\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col).\u001b[34m__name__\u001b[39m},\n\u001b[32m   5173\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m5174\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [DATATYPE_MISMATCH.CAST_WITHOUT_SUGGESTION] Cannot resolve \"CAST(tpep_dropoff_datetime AS BIGINT)\" due to data type mismatch: cannot cast \"TIMESTAMP_NTZ\" to \"BIGINT\".;\n'Project [VendorID#1174, tpep_pickup_datetime#1175, tpep_dropoff_datetime#1176, passenger_count#1177L, trip_distance#1178, RatecodeID#1179L, store_and_fwd_flag#1180, PULocationID#1181, DOLocationID#1182, payment_type#1183L, fare_amount#1184, extra#1185, mta_tax#1186, tip_amount#1187, tolls_amount#1188, improvement_surcharge#1189, total_amount#1190, congestion_surcharge#1191, Airport_fee#1192, (cast(tpep_dropoff_datetime#1176 as bigint) - cast(tpep_pickup_datetime#1175 as bigint)) AS duracion_segundos#1946]\n+- Relation [VendorID#1174,tpep_pickup_datetime#1175,tpep_dropoff_datetime#1176,passenger_count#1177L,trip_distance#1178,RatecodeID#1179L,store_and_fwd_flag#1180,PULocationID#1181,DOLocationID#1182,payment_type#1183L,fare_amount#1184,extra#1185,mta_tax#1186,tip_amount#1187,tolls_amount#1188,improvement_surcharge#1189,total_amount#1190,congestion_surcharge#1191,Airport_fee#1192] parquet\n"
     ]
    }
   ],
   "source": [
    "# Restar timestamps nos da un \"Intervalo\". \n",
    "# Necesitamos convertirlo a segundos usando .cast(\"long\")\n",
    "df_con_duracion = df_spark.withColumn(\"duracion_segundos\",\n",
    "    (F.col(\"tpep_dropoff_datetime\").cast(\"long\") - F.col(\"tpep_pickup_datetime\").cast(\"long\"))\n",
    ")\n",
    "\n",
    "df_con_duracion.select(\"duracion_segundos\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fe1748-94e3-4ec4-9ed7-8227cb8b578b",
   "metadata": {},
   "source": [
    "En la Celda anterior, te encontraste con un `AnalysisException`. ¡Felicidades! Este es tu primer error de *tipos de datos* en Spark.\n",
    "\n",
    "**El Error:** `AnalysisException: [DATATYPE_MISMATCH.CAST_WITHOUT_SUGGESTION] ... cannot cast \"TIMESTAMP_NTZ\" to \"BIGINT\"`\n",
    "\n",
    "**Análisis del Error:**\n",
    "* El error nos dice que Spark no sabe cómo convertir un `TIMESTAMP_NTZ` (un *timestamp* o marca de tiempo, sin zona horaria) directamente a un `BIGINT` (un número entero largo, como `long`).\n",
    "* Tu código `F.col(...).cast(\"long\")` es lo que generó este error.\n",
    "* En versiones antiguas de Spark, esto a veces funcionaba, ya que `cast(\"long\")` era un atajo para \"dame los segundos *epoch*\". En las versiones modernas, Spark es mucho más estricto con los tipos de datos para evitar ambigüedades. No sabe si quieres los segundos, los milisegundos o los microsegundos.\n",
    "\n",
    "**La Solución:**\n",
    "Para obtener la duración en segundos, primero debemos convertir el *timestamp* a un tipo numérico que represente los segundos *epoch*. El tipo de dato correcto para esto es **`double`** (un número de punto flotante de alta precisión).\n",
    "\n",
    "Aquí está la celda corregida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f148cd74-46fa-422a-a401-f4f547881383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------+\n",
      "|New DropOff|tpep_dropoff_datetime|\n",
      "+-----------+---------------------+\n",
      "| 1704071863|  2024-01-01 01:17:43|\n",
      "| 1704067776|  2024-01-01 00:09:36|\n",
      "| 1704069301|  2024-01-01 00:35:01|\n",
      "| 1704069896|  2024-01-01 00:44:56|\n",
      "| 1704070377|  2024-01-01 00:52:57|\n",
      "+-----------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_time_review = df_spark.withColumn(\"New DropOff\",F.unix_timestamp(F.col(\"tpep_dropoff_datetime\")))\n",
    "df_time_review.select(\"New DropOff\",\"tpep_dropoff_datetime\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ef505f0-1482-425f-931d-1b6706f006e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      " |-- New DropOff: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_time_review.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6b35197-bc4a-4432-87a0-1e2cfa918667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cálculo de duración exitoso:\n",
      "+--------------------+---------------------+-----------------+------------------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|duracion_segundos|  duracion_minutos|\n",
      "+--------------------+---------------------+-----------------+------------------+\n",
      "| 2024-01-01 00:57:55|  2024-01-01 01:17:43|             1188|              19.8|\n",
      "| 2024-01-01 00:03:00|  2024-01-01 00:09:36|              396|               6.6|\n",
      "| 2024-01-01 00:17:06|  2024-01-01 00:35:01|             1075|17.916666666666668|\n",
      "| 2024-01-01 00:36:38|  2024-01-01 00:44:56|              498|               8.3|\n",
      "| 2024-01-01 00:46:51|  2024-01-01 00:52:57|              366|               6.1|\n",
      "+--------------------+---------------------+-----------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Viajes con duración negativa o cero: 870\n"
     ]
    }
   ],
   "source": [
    "# Celda 13 (Corregida): Cálculo de Duración\n",
    "#\n",
    "# Corregimos el error anterior. En lugar de .cast(), usamos\n",
    "# la función explícita F.unix_timestamp() para convertir\n",
    "# el timestamp en un número (segundos epoch).\n",
    "\n",
    "df_con_duracion = df_spark.withColumn(\"duracion_segundos\",\n",
    "    (F.unix_timestamp(F.col(\"tpep_dropoff_datetime\")) - F.unix_timestamp(F.col(\"tpep_pickup_datetime\")))\n",
    ")\n",
    "\n",
    "# Ahora podemos hacer aritmética simple\n",
    "df_con_duracion = df_con_duracion.withColumn(\"duracion_minutos\",\n",
    "    (F.col(\"duracion_segundos\") / 60).cast(\"double\") # Hacemos cast del *resultado*\n",
    ")\n",
    "\n",
    "print(\"Cálculo de duración exitoso:\")\n",
    "df_con_duracion.select(\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"duracion_segundos\", \"duracion_minutos\").show(5)\n",
    "\n",
    "# Limpiemos los viajes con duración negativa (errores de datos)\n",
    "print(f\"Viajes con duración negativa o cero: {df_con_duracion.filter(F.col('duracion_segundos') <= 0).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7b4e1b-4982-4738-aca5-038b4681da71",
   "metadata": {},
   "source": [
    "**Alternativa - `F.datediff()`**\n",
    "\n",
    "Si solo te importara la diferencia en *días* (no segundos), podrías usar `datediff`. Nota que esta función opera sobre *fechas* (`date`), no sobre *marcas de tiempo* (`timestamp`), así que primero debemos hacer un `cast(\"date\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "660fb0f4-9bab-48f1-85c7-11de6bad305b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diferencia en DÍAS:\n",
      "+--------------------+---------------------+-------------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|dias_de_viaje|\n",
      "+--------------------+---------------------+-------------+\n",
      "| 2024-01-01 00:57:55|  2024-01-01 01:17:43|            0|\n",
      "| 2024-01-01 00:03:00|  2024-01-01 00:09:36|            0|\n",
      "| 2024-01-01 00:17:06|  2024-01-01 00:35:01|            0|\n",
      "| 2024-01-01 00:36:38|  2024-01-01 00:44:56|            0|\n",
      "| 2024-01-01 00:46:51|  2024-01-01 00:52:57|            0|\n",
      "+--------------------+---------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dias = df_spark.withColumn(\"dias_de_viaje\",\n",
    "    F.datediff(\n",
    "        F.col(\"tpep_dropoff_datetime\").cast(\"date\"),\n",
    "        F.col(\"tpep_pickup_datetime\").cast(\"date\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nDiferencia en DÍAS:\")\n",
    "df_dias.select(\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"dias_de_viaje\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e3ba81-92f8-493a-8829-11f23817e88e",
   "metadata": {},
   "source": [
    "### Parte 4: Agregaciones (El `groupBy` y `agg`)\n",
    "\n",
    "Aquí volvemos al `groupBy`, pero con más funciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e28b0d-a56c-48f6-9565-647099b570d8",
   "metadata": {},
   "source": [
    "**`groupBy()` con Múltiples Agregaciones**\n",
    "\n",
    "En lugar de solo `avg`, podemos pedirle a Spark que calcule todo a la vez: `avg`, `sum`, `max`, `min`, `count`, `countDistinct` (conteo de valores únicos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "532d65b1-52ac-4301-8e2f-9a62e4c9a618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agregación completa por Zona de Recogida:\n",
      "+------------+-------------+------------------+--------------------+----------------+-----------------+\n",
      "|PULocationID|conteo_viajes|    promedio_total|          suma_total|distancia_maxima|tipos_pago_unicos|\n",
      "+------------+-------------+------------------+--------------------+----------------+-----------------+\n",
      "|         132|       145240| 76.57620855134614|1.1121928529997513E7|        10879.28|                5|\n",
      "|         161|       143471|23.482411149291384|   3369045.009999984|        38202.66|                5|\n",
      "|         237|       142708|19.453676878661152|  2776195.3199999756|           971.8|                5|\n",
      "|         236|       136465| 20.00189477155298|  2729558.5699999775|           58.81|                5|\n",
      "|         162|       106717| 22.88040002998577|   2441727.649999991|           71.18|                5|\n",
      "|         230|       106324| 26.26924946390265|  2793051.6799999853|            80.0|                5|\n",
      "|         186|       104523|23.640956153191134|   2471023.659999997|            94.0|                5|\n",
      "|         142|       104080|20.998902863182096|  2185565.8099999926|           58.78|                5|\n",
      "|         138|        89533|   65.014798565891|   5820969.959999919|          176.43|                5|\n",
      "|         239|        88474|20.934708275877732|  1852177.3800000064|        15015.12|                5|\n",
      "+------------+-------------+------------------+--------------------+----------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agrupemos por 'PULocationID' (Zona de recogida)\n",
    "# Calculemos múltiples estadísticas sobre esos viajes\n",
    "df_agregado = df_spark.groupBy(\"PULocationID\").agg(\n",
    "    F.count(\"*\").alias(\"conteo_viajes\"),\n",
    "    F.avg(\"total_amount\").alias(\"promedio_total\"),\n",
    "    F.sum(\"total_amount\").alias(\"suma_total\"),\n",
    "    F.max(\"trip_distance\").alias(\"distancia_maxima\"),\n",
    "    F.countDistinct(\"payment_type\").alias(\"tipos_pago_unicos\")\n",
    ")\n",
    "\n",
    "print(\"Agregación completa por Zona de Recogida:\")\n",
    "df_agregado.orderBy(F.col(\"conteo_viajes\").desc()).show(10) # Ordenamos por los más populares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa636a2e-066b-4bd2-89de-c59dd21963f8",
   "metadata": {},
   "source": [
    "**Agregación Global (Sin `groupBy`)**\n",
    "\n",
    "¿Qué pasa si quieres el promedio de `total_amount` de *todo* el DataFrame? No necesitas un `groupBy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89ca00bf-6526-4b8b-a680-76d016ac2074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estadísticas globales de TODO el dataset:\n",
      "+---------------------+--------------------+---------------+\n",
      "|promedio_total_global|total_tarifas_global|viaje_mas_largo|\n",
      "+---------------------+--------------------+---------------+\n",
      "|   26.801504770952707|5.3882224760004714E7|       312722.3|\n",
      "+---------------------+--------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Usamos .agg() directamente sobre el DataFrame\n",
    "df_stats_globales = df_spark.agg(\n",
    "    F.avg(\"total_amount\").alias(\"promedio_total_global\"),\n",
    "    F.sum(\"fare_amount\").alias(\"total_tarifas_global\"),\n",
    "    F.max(\"trip_distance\").alias(\"viaje_mas_largo\")\n",
    ")\n",
    "\n",
    "print(\"Estadísticas globales de TODO el dataset:\")\n",
    "df_stats_globales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8a11ba-97b1-4bca-9f8c-0db7f6db0202",
   "metadata": {},
   "source": [
    "### Parte 5: Uniendo DataFrames (El `join`)\n",
    "\n",
    "El `join` es fundamental. Vamos a crear un DataFrame \"diccionario\" para nuestros `payment_type` y a unirlos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c875f985-ff81-48ac-9279-552db8f8d9f0",
   "metadata": {},
   "source": [
    "**Creando un DataFrame \"Diccionario\"**\n",
    "\n",
    "Crearemos un pequeño DataFrame de Spark desde cero para mapear los IDs de pago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4876df9-5438-4c26-a7c3-8c49e9524ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2024-01-01 00:57:55|  2024-01-01 01:17:43|              1|         1.72|         1|                 N|         186|          79|           2|       17.7|  1.0|    0.5|       0.0|         0.0|                  1.0|        22.7|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:03:00|  2024-01-01 00:09:36|              1|          1.8|         1|                 N|         140|         236|           1|       10.0|  3.5|    0.5|      3.75|         0.0|                  1.0|       18.75|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:17:06|  2024-01-01 00:35:01|              1|          4.7|         1|                 N|         236|          79|           1|       23.3|  3.5|    0.5|       3.0|         0.0|                  1.0|        31.3|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:36:38|  2024-01-01 00:44:56|              1|          1.4|         1|                 N|          79|         211|           1|       10.0|  3.5|    0.5|       2.0|         0.0|                  1.0|        17.0|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:46:51|  2024-01-01 00:52:57|              1|          0.8|         1|                 N|         211|         148|           1|        7.9|  3.5|    0.5|       3.2|         0.0|                  1.0|        16.1|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:54:08|  2024-01-01 01:26:31|              1|          4.7|         1|                 N|         148|         141|           1|       29.6|  3.5|    0.5|       6.9|         0.0|                  1.0|        41.5|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:49:44|  2024-01-01 01:15:47|              2|        10.82|         1|                 N|         138|         181|           1|       45.7|  6.0|    0.5|      10.0|         0.0|                  1.0|       64.95|                 0.0|       1.75|\n",
      "|       1| 2024-01-01 00:30:40|  2024-01-01 00:58:40|              0|          3.0|         1|                 N|         246|         231|           2|       25.4|  3.5|    0.5|       0.0|         0.0|                  1.0|        30.4|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:26:01|  2024-01-01 00:54:12|              1|         5.44|         1|                 N|         161|         261|           2|       31.0|  1.0|    0.5|       0.0|         0.0|                  1.0|        36.0|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:28:08|  2024-01-01 00:29:16|              1|         0.04|         1|                 N|         113|         113|           2|        3.0|  1.0|    0.5|       0.0|         0.0|                  1.0|         8.0|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:35:22|  2024-01-01 00:41:41|              2|         0.75|         1|                 N|         107|         137|           1|        7.9|  1.0|    0.5|       0.0|         0.0|                  1.0|        12.9|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:25:00|  2024-01-01 00:34:03|              2|          1.2|         1|                 N|         158|         246|           1|       14.9|  3.5|    0.5|      3.95|         0.0|                  1.0|       23.85|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:35:16|  2024-01-01 01:11:52|              2|          8.2|         1|                 N|         246|         190|           1|       59.0|  3.5|    0.5|     14.15|        6.94|                  1.0|       85.09|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:43:27|  2024-01-01 00:47:11|              2|          0.4|         1|                 N|          68|          90|           1|        5.8|  3.5|    0.5|      1.25|         0.0|                  1.0|       12.05|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:51:53|  2024-01-01 00:55:43|              1|          0.8|         1|                 N|          90|          68|           2|        6.5|  3.5|    0.5|       0.0|         0.0|                  1.0|        11.5|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:50:09|  2024-01-01 01:03:57|              1|          5.0|         1|                 N|         132|         216|           2|       21.2| 2.75|    0.5|       0.0|         0.0|                  1.0|       25.45|                 0.0|       1.75|\n",
      "|       1| 2024-01-01 00:41:06|  2024-01-01 00:53:42|              1|          1.5|         1|                 N|         164|          79|           1|       12.8|  3.5|    0.5|      4.45|         0.0|                  1.0|       22.25|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:52:09|  2024-01-01 00:52:28|              1|          0.0|         1|                 N|         237|         237|           2|        3.0|  1.0|    0.5|       0.0|         0.0|                  1.0|         8.0|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:56:38|  2024-01-01 01:03:17|              1|          1.5|         1|                 N|         141|         263|           1|        9.3|  1.0|    0.5|       3.0|         0.0|                  1.0|        17.3|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:32:34|  2024-01-01 00:49:33|              1|         2.57|         1|                 N|         161|         263|           1|       17.7|  1.0|    0.5|      10.0|         0.0|                  1.0|        32.7|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ab5d77b-2af1-49ec-bdb6-9363ea41ef9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuestro DataFrame diccionario COMPLETO:\n",
      "+----------+------------------+\n",
      "|payment_id|       nombre_pago|\n",
      "+----------+------------------+\n",
      "|         0|       Desconocido|\n",
      "|         1|Tarjeta de Crédito|\n",
      "|         2|          Efectivo|\n",
      "|         3|         Sin Cargo|\n",
      "|         4|           Disputa|\n",
      "|         5|             Vacío|\n",
      "|         6|              Nulo|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Definimos los datos y el esquema (incluyendo el '0')\n",
    "# (Una búsqueda rápida en Google sobre el dataset nos dice qué son 0, 5, 6)\n",
    "datos_pago = [(0, \"Desconocido\"), # ¡Lo encontramos gracias al groupBy!\n",
    "              (1, \"Tarjeta de Crédito\"),\n",
    "              (2, \"Efectivo\"),\n",
    "              (3, \"Sin Cargo\"),\n",
    "              (4, \"Disputa\"),\n",
    "              (5, \"Vacío\"),\n",
    "              (6, \"Nulo\")]\n",
    "esquema_pago = [\"payment_id\", \"nombre_pago\"]\n",
    "\n",
    "# 2. Creamos el DataFrame\n",
    "df_diccionario_pagos = spark.createDataFrame(datos_pago, schema=esquema_pago)\n",
    "\n",
    "print(\"Nuestro DataFrame diccionario COMPLETO:\")\n",
    "df_diccionario_pagos.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ebbbd4-b756-4f29-bb10-aa4d4a7ea5a2",
   "metadata": {},
   "source": [
    "**`join()` - Uniendo los DataFrames**\n",
    "\n",
    "Ahora, unamos nuestro `df_spark` principal con nuestro `df_diccionario_pagos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18f4d6b8-7077-4472-851a-f883f39f8c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame unido con nombres de pago:\n",
      "+---------------+------------+------------------+\n",
      "|passenger_count|total_amount|       nombre_pago|\n",
      "+---------------+------------+------------------+\n",
      "|              1|       18.75|Tarjeta de Crédito|\n",
      "|              1|        31.3|Tarjeta de Crédito|\n",
      "|              1|        17.0|Tarjeta de Crédito|\n",
      "|              1|        16.1|Tarjeta de Crédito|\n",
      "|              1|        41.5|Tarjeta de Crédito|\n",
      "|              2|       64.95|Tarjeta de Crédito|\n",
      "|              2|        12.9|Tarjeta de Crédito|\n",
      "|              1|        22.7|          Efectivo|\n",
      "|              0|        30.4|          Efectivo|\n",
      "|              1|        36.0|          Efectivo|\n",
      "+---------------+------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# La sintaxis del join es:\n",
    "# df_izquierdo.join(df_derecho, <condición_del_join>, <tipo_de_join>)\n",
    "df_unido = df_spark.join(\n",
    "    df_diccionario_pagos,\n",
    "    df_spark.payment_type == df_diccionario_pagos.payment_id, # La condición\n",
    "    \"left_outer\" # Tipo de join (left, inner, right, full_outer)\n",
    ")\n",
    "\n",
    "print(\"DataFrame unido con nombres de pago:\")\n",
    "df_unido.select(\"passenger_count\", \"total_amount\", \"nombre_pago\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cada204-24cd-4a41-868e-2404de51819f",
   "metadata": {},
   "source": [
    "### Parte 6: El Benchmark (¡Ahora sí!)\n",
    "\n",
    "Ahora repetimos un ejercicio anterior, pero con todo lo que sabemos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb33ede-f219-4d3b-b1ef-bdc9f4c7ce23",
   "metadata": {},
   "source": [
    "**El Benchmark (Desbalanceado vs. Balanceado)**\n",
    "\n",
    "Repetimos el experimento de la \"Clase 2\", pero ahora entendemos cada función (`filter`, `withColumn`, `groupBy`, `agg`, `orderBy`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18b0ab20-3bf2-45a9-85d0-e11911a53cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ejecutando Benchmark 1 (Desbalanceado, 3 núcleos activos) ---\n",
      "+------------+----------+--------------------+-------------+\n",
      "|payment_type|tipo_viaje|    propina_promedio|conteo_viajes|\n",
      "+------------+----------+--------------------+-------------+\n",
      "|           1|     Medio|   6.008564521889089|       938015|\n",
      "|           2|     Medio|0.002486501326988...|       163905|\n",
      "|           0|     Medio|   2.234879677758519|        67527|\n",
      "|           1|     Largo|  15.338880528457176|        22859|\n",
      "|           4|     Medio| 0.06863114231014676|        15670|\n",
      "|           2|     Largo|0.032797917470111834|         5186|\n",
      "|           3|     Medio|0.028834973166368513|         4472|\n",
      "|           4|     Largo|  0.3381708945260347|          749|\n",
      "|           0|     Largo|   10.91443762781186|          489|\n",
      "|           3|     Largo| 0.13790960451977402|          177|\n",
      "+------------+----------+--------------------+-------------+\n",
      "\n",
      "CPU times: user 1.8 ms, sys: 957 μs, total: 2.76 ms\n",
      "Wall time: 514 ms\n",
      "\n",
      "--- Ejecutando Benchmark 2 (Balanceado, 12 núcleos activos) ---\n",
      "+------------+----------+--------------------+-------------+\n",
      "|payment_type|tipo_viaje|    propina_promedio|conteo_viajes|\n",
      "+------------+----------+--------------------+-------------+\n",
      "|           1|     Medio|   6.008564521889595|       938015|\n",
      "|           2|     Medio|0.002486501326988...|       163905|\n",
      "|           0|     Medio|   2.234879677758525|        67527|\n",
      "|           1|     Largo|  15.338880528457011|        22859|\n",
      "|           4|     Medio| 0.06863114231014678|        15670|\n",
      "|           2|     Largo|0.032797917470111834|         5186|\n",
      "|           3|     Medio|0.028834973166368513|         4472|\n",
      "|           4|     Largo|  0.3381708945260347|          749|\n",
      "|           0|     Largo|  10.914437627811862|          489|\n",
      "|           3|     Largo| 0.13790960451977402|          177|\n",
      "+------------+----------+--------------------+-------------+\n",
      "\n",
      "CPU times: user 1.83 ms, sys: 405 μs, total: 2.23 ms\n",
      "Wall time: 212 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: int, tpep_pickup_datetime: timestamp_ntz, tpep_dropoff_datetime: timestamp_ntz, passenger_count: bigint, trip_distance: double, RatecodeID: bigint, store_and_fwd_flag: string, PULocationID: int, DOLocationID: int, payment_type: bigint, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, improvement_surcharge: double, total_amount: double, congestion_surcharge: double, Airport_fee: double]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 1. Definir la consulta compleja ---\n",
    "def analizar_viajes_completo(dataframe):\n",
    "    return dataframe.filter(\n",
    "        (F.col(\"trip_distance\") > 2) & (F.col(\"trip_distance\") < 100)\n",
    "    ).withColumn(\"tipo_viaje\",\n",
    "        F.when(F.col(\"trip_distance\") > 20, F.lit(\"Largo\"))\n",
    "         .otherwise(F.lit(\"Medio\"))\n",
    "    ).groupBy(\"payment_type\", \"tipo_viaje\").agg(\n",
    "        F.avg(\"tip_amount\").alias(\"propina_promedio\"),\n",
    "        F.count(\"*\").alias(\"conteo_viajes\")\n",
    "    ).orderBy(F.col(\"conteo_viajes\").desc())\n",
    "\n",
    "\n",
    "# --- 2. Preparar los DataFrames (como lo hiciste) ---\n",
    "df_spark.cache()\n",
    "df_spark.count() # Forzar cacheo\n",
    "\n",
    "df_reparticionado = df_spark.repartition(12)\n",
    "df_reparticionado.cache()\n",
    "df_reparticionado.count() # Forzar repartition y cacheo\n",
    "\n",
    "# --- 3. Ejecutar Benchmarks ---\n",
    "print(\"--- Ejecutando Benchmark 1 (Desbalanceado, 3 núcleos activos) ---\")\n",
    "%time analizar_viajes_completo(df_spark).show()\n",
    "\n",
    "print(\"\\n--- Ejecutando Benchmark 2 (Balanceado, 12 núcleos activos) ---\")\n",
    "%time analizar_viajes_completo(df_reparticionado).show()\n",
    "\n",
    "# Liberamos memoria\n",
    "df_spark.unpersist()\n",
    "df_reparticionado.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ec324-92c6-44f6-84f9-420d05cf8053",
   "metadata": {},
   "source": [
    "¡Aquí es donde todo el trabajo conceptual da sus frutos! Los resultados de tu benchmark son la prueba perfecta de por qué la optimización importa.\n",
    "\n",
    "**Tus Resultados:**\n",
    "* **Benchmark 1 (Desbalanceado):** `Wall time: 542 ms`\n",
    "* **Benchmark 2 (Balanceado):** `Wall time: 303 ms`\n",
    "\n",
    "**Análisis de Rendimiento:**\n",
    "* Al ejecutar `df_spark.repartition(12)`, hiciste que la consulta fuera **¡un 79% más rápida!** (`542 / 303 = 1.79`).\n",
    "* **¿Por qué?**\n",
    "    * En el Benchmark 1, las primeras etapas (el `filter` y `withColumn`) se ejecutaron en solo **3 núcleos**, creando un \"cuello de botella\". Los otros 9 núcleos estaban esperando.\n",
    "    * En el Benchmark 2, esas mismas etapas se ejecutaron en **12 núcleos en paralelo**.\n",
    "* **Driver vs. Executors:**\n",
    "    * Fíjate en el `CPU times` (tiempo del Driver): `3.37 ms` vs `6.86 ms`.\n",
    "    * Esto muestra que el Driver (tu notebook) tuvo que \"trabajar\" un poquito más en el Benchmark 2. ¿Por qué? ¡Porque tuvo que coordinar a 12 trabajadores en lugar de a 3!\n",
    "    * Pero el `Wall time` (tiempo total de reloj) es lo que importa, y se redujo drásticamente.\n",
    "\n",
    "**¡Felicidades!** Has demostrado empíricamente cómo una optimización de distribución de datos impacta directamente el rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7182baa-7a44-431f-9dbc-88c51b11ca99",
   "metadata": {},
   "source": [
    "Hemos cubierto:\n",
    "* **Selección:** `select`, `selectExpr`\n",
    "* **Filtrado:** `filter`, `where`, `like`, `isin`\n",
    "* **Manipulación:** `withColumn`, `withColumnRenamed`, `drop`\n",
    "* **Funciones `F`:** `lit`, `when`, `otherwise`, `isNull`, `year`, `month`, `dayofweek`, `hour`, `cast`\n",
    "* **Manejo de Nulos:** `fillna`\n",
    "* **Agregaciones:** `groupBy`, `agg`, `avg`, `sum`, `max`, `count`, `countDistinct`\n",
    "* **Uniones:** `join`, `createDataFrame`\n",
    "* **Acciones:** `show`, `count`, `printSchema` (y las de la clase pasada: `collect`, `write`, `toPandas`)\n",
    "\n",
    "\n",
    "Con esto conformamos un \"Cookbook\" funcional para empezar a trabajar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddaeb9f-e4a7-4177-862a-865fe8c2eebc",
   "metadata": {},
   "source": [
    "### Parte 6: El Siguiente Nivel - Spark SQL y UDFs\n",
    "\n",
    "Has dominado la \"API de DataFrame\" (el estilo Python). Pero Spark tiene otra cara: **Spark SQL**.\n",
    "\n",
    "Además, ¿qué pasa si la función que quieres no existe en `F`? Creas la tuya: una **UDF** (User Defined Function)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f7e756-9e86-4687-92f2-017f5591467a",
   "metadata": {},
   "source": [
    "**`createOrReplaceTempView()` - Hablando SQL**\n",
    "\n",
    "Podemos tomar *cualquier* DataFrame (como nuestro `df_unido_completo`) y registrarlo como una \"tabla\" temporal de SQL. Una vez hecho, ¡puedes usar sintaxis SQL pura!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8946e6b0-aacd-4a3a-9a83-0f1900bdac74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡El mismo resultado, pero generado con 100% SQL!\n",
      "+------------+------------------+----------+--------------------+-------------+\n",
      "|payment_type|       nombre_pago|tipo_viaje|    propina_promedio|conteo_viajes|\n",
      "+------------+------------------+----------+--------------------+-------------+\n",
      "|           1|Tarjeta de Crédito|     Medio|    6.00856452188944|       938015|\n",
      "|           2|          Efectivo|     Medio|0.002486501326988...|       163905|\n",
      "|           0|       Desconocido|     Medio|   2.234879677758519|        67527|\n",
      "|           1|Tarjeta de Crédito|     Largo|  15.338880528457112|        22859|\n",
      "|           4|           Disputa|     Medio| 0.06863114231014678|        15670|\n",
      "|           2|          Efectivo|     Largo|0.032797917470111834|         5186|\n",
      "|           3|         Sin Cargo|     Medio| 0.02883497316636852|         4472|\n",
      "|           4|           Disputa|     Largo|  0.3381708945260347|          749|\n",
      "|           0|       Desconocido|     Largo|   10.91443762781186|          489|\n",
      "|           3|         Sin Cargo|     Largo| 0.13790960451977402|          177|\n",
      "+------------+------------------+----------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Registramos nuestro DataFrame como una tabla SQL temporal\n",
    "df_unido.createOrReplaceTempView(\"taxis\")\n",
    "\n",
    "# 2. ¡Escribimos SQL!\n",
    "# Esto hace LO MISMO que nuestro \"analizar_viajes_completo\"\n",
    "df_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        payment_type, \n",
    "        nombre_pago,\n",
    "        CASE \n",
    "            WHEN trip_distance > 20 THEN 'Largo'\n",
    "            ELSE 'Medio'\n",
    "        END AS tipo_viaje,\n",
    "        AVG(tip_amount) AS propina_promedio,\n",
    "        COUNT(*) AS conteo_viajes\n",
    "    FROM taxis\n",
    "    WHERE trip_distance > 2 AND trip_distance < 100\n",
    "    GROUP BY payment_type, nombre_pago, tipo_viaje\n",
    "    ORDER BY conteo_viajes DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"¡El mismo resultado, pero generado con 100% SQL!\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff908147-b6d6-4cca-bd4b-f64af729d63f",
   "metadata": {},
   "source": [
    "**`F.udf()` - Creando tus propias funciones**\n",
    "\n",
    "¿Qué pasa si queremos una función compleja que no existe en Spark? Por ejemplo, una que clasifique la propina en \"Baja\", \"Media\", \"Alta\".\n",
    "\n",
    "**Advertencia:** Las UDF son *lentas*. Spark tiene que enviar los datos de su motor optimizado (JVM) a Python, ejecutar tu función, y devolver el resultado. **Siempre prefiere funciones `F` nativas si puedes.** Pero a veces, son inevitables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee1981ae-ab0b-4f51-940d-1017005c16c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame con nuestra UDF personalizada:\n",
      "+----------+------------+-----------------+\n",
      "|tip_amount|total_amount|categoria_propina|\n",
      "+----------+------------+-----------------+\n",
      "|       0.0|        22.7|          Ninguna|\n",
      "|      3.75|       18.75|            Media|\n",
      "|       3.0|        31.3|             Baja|\n",
      "|       2.0|        17.0|             Baja|\n",
      "|       3.2|        16.1|            Media|\n",
      "|       6.9|        41.5|            Media|\n",
      "|      10.0|       64.95|            Media|\n",
      "|       0.0|        30.4|          Ninguna|\n",
      "|       0.0|        36.0|          Ninguna|\n",
      "|       0.0|         8.0|          Ninguna|\n",
      "+----------+------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------------+-------+\n",
      "|categoria_propina|  count|\n",
      "+-----------------+-------+\n",
      "|            Media|1638576|\n",
      "|          Ninguna| 710378|\n",
      "|             Baja| 599218|\n",
      "|             Alta|  16036|\n",
      "|              N/A|    416|\n",
      "+-----------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# 1. Definimos una función de Python normal\n",
    "def clasificar_propina(propina, total):\n",
    "    if total is None or total == 0:\n",
    "        return \"N/A\"\n",
    "    if propina is None:\n",
    "        propina = 0\n",
    "    \n",
    "    pct = (propina / total) * 100\n",
    "    \n",
    "    if pct > 25:\n",
    "        return \"Alta\"\n",
    "    elif pct > 15:\n",
    "        return \"Media\"\n",
    "    elif pct > 0:\n",
    "        return \"Baja\"\n",
    "    else:\n",
    "        return \"Ninguna\"\n",
    "\n",
    "# 2. \"Envolvemos\" nuestra función de Python en una UDF de Spark\n",
    "# Le decimos a Spark el tipo de dato que nuestra función va a devolver\n",
    "clasificar_propina_udf = F.udf(clasificar_propina, StringType())\n",
    "\n",
    "# 3. Usamos la UDF como si fuera una función F\n",
    "df_con_udf = df_spark.withColumn(\"categoria_propina\", \n",
    "    clasificar_propina_udf(F.col(\"tip_amount\"), F.col(\"total_amount\"))\n",
    ")\n",
    "\n",
    "print(\"DataFrame con nuestra UDF personalizada:\")\n",
    "df_con_udf.select(\"tip_amount\", \"total_amount\", \"categoria_propina\").show(10)\n",
    "\n",
    "# ¡Ahora podemos contar las categorías!\n",
    "df_con_udf.groupBy(\"categoria_propina\").count().orderBy(F.col(\"count\").desc()).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
