{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bd3ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, broadcast, current_timestamp, regexp_replace\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, TimestampType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e935c46f",
   "metadata": {},
   "source": [
    "# Inicialización de SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baf2b2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sesión de Spark iniciada con soporte para Kafka y Delta Lake.\n"
     ]
    }
   ],
   "source": [
    "# Paquetes requeridos para Kafka y Delta Lake (deben ser coherentes con la versión de Spark 3.5.1)\n",
    "SPARK_PACKAGES = (\n",
    "    \"io.delta:delta-spark_2.12:3.1.0,\"\n",
    "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\"\n",
    ")\n",
    "SPARK_MASTER = \"spark://0.0.0.0:7077\" \n",
    "\n",
    "# Configuración de Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BronzeToSilverStreaming\") \\\n",
    "    .master(SPARK_MASTER) \\\n",
    "    .config(\"spark.jars.packages\", SPARK_PACKAGES) \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"Sesión de Spark iniciada con soporte para Kafka y Delta Lake.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0036ca6e",
   "metadata": {},
   "source": [
    "# Ingesta: Leer stream de Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4963943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream de Kafka configurado.\n"
     ]
    }
   ],
   "source": [
    "# Esquema de los datos reales del productor (8 campos clave + 6 auxiliares)\n",
    "CONTRACT_SCHEMA = StructType([\n",
    "    # Campos clave para ML\n",
    "    StructField(\"id_contrato\", StringType(), True),\n",
    "    StructField(\"objeto_contrato\", StringType(), True),\n",
    "    StructField(\"entidad\", StringType(), True),\n",
    "    StructField(\"codigo_unspsc\", StringType(), True), \n",
    "    StructField(\"duracion_dias\", LongType(), True),\n",
    "    StructField(\"valor_contrato\", DoubleType(), True),\n",
    "    StructField(\"fecha_firma\", StringType(), True),\n",
    "    StructField(\"departamento\", StringType(), True),\n",
    "    \n",
    "    # Columnas ruidosas / auxiliares (Serán eliminadas)\n",
    "    StructField(\"nit_entidad\", StringType(), True), \n",
    "    StructField(\"localizacion\", StringType(), True),\n",
    "    StructField(\"sector\", StringType(), True), \n",
    "    StructField(\"es_pyme\", StringType(), True),\n",
    "    StructField(\"valor_facturado\", StringType(), True), # Aunque es numérico, puede venir sucio\n",
    "    StructField(\"urlproceso\", StringType(), True),\n",
    "])\n",
    "\n",
    "# 1. Tarea: Leer el stream de Kafka\n",
    "kafka_stream = (spark.readStream\n",
    "    .format(\"kafka\") \n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\")\n",
    "    .option(\"subscribe\", \"contratos-publicos\")\n",
    "    .option(\"startingOffsets\", \"earliest\") # Para procesar datos desde el inicio (si el productor ya corrió)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "print(\"Stream de Kafka configurado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d0c978",
   "metadata": {},
   "source": [
    "# Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7088408d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame de Regiones (Broadcast) listo para el Join.\n"
     ]
    }
   ],
   "source": [
    "# Definimos los departamentos que forman parte del Eje Cafetero para el filtro\n",
    "regiones_eje_cafetero = [\n",
    "    (\"Antioquia\", \"Eje Cafetero\"), (\"Caldas\", \"Eje Cafetero\"), \n",
    "    (\"Quindio\", \"Eje Cafetero\"), (\"Risaralda\", \"Eje Cafetero\"), \n",
    "    (\"Tolima\", \"Eje Cafetero\"), (\"Valle del Cauca\", \"Eje Cafetero\")\n",
    "]\n",
    "df_regiones = spark.createDataFrame(regiones_eje_cafetero).toDF(\"departamento_join\", \"macrorregion_turistica\")\n",
    "\n",
    "# Aplicamos Broadcasting para optimizar el JOIN (Broadcasting Join)\n",
    "df_regiones_broadcast = broadcast(df_regiones)\n",
    "print(\"DataFrame de Regiones (Broadcast) listo para el Join.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a26204",
   "metadata": {},
   "source": [
    "# Persistencia en Delta-Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d011fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_silver = (kafka_stream \\\n",
    "    # 2. Explosión de Metadatos\n",
    "    .withColumn(\"value_content\", from_json(col(\"value\").cast(\"string\"), CONTRACT_SCHEMA)) \\\n",
    "    .select(\n",
    "        col(\"value_content.*\"),\n",
    "        col(\"timestamp\").alias(\"kafka_ingestion_time\"), # Metadato de Kafka\n",
    "        col(\"offset\").alias(\"kafka_offset\")             # Metadato de Kafka\n",
    "    ) \\\n",
    "    \n",
    "    # 3. Tarea: Limpieza (Eliminación de Redundantes)\n",
    "    .drop(\"nit_entidad\", \"localizacion\", \"sector\", \"es_pyme\", \"valor_facturado\", \"urlproceso\") \\\n",
    "    \n",
    "    # 3. Tarea: Cruce con Regiones (Broadcasting Join)\n",
    "    .join(\n",
    "        df_regiones_broadcast,\n",
    "        on=df_regiones_broadcast.departamento_join == col(\"departamento\"),\n",
    "        how=\"inner\" # INNER JOIN garantiza que solo pasen los del Eje Cafetero\n",
    "    ) \\\n",
    "    .drop(\"departamento_join\") \\\n",
    "    \n",
    "    # Limpieza final de valores (solo si es necesario para asegurar tipos, aunque ya se hizo en el productor)\n",
    "    .withColumn(\"departamento\", col(\"departamento\").cast(StringType())) \\\n",
    "    .withColumn(\"processing_time\", current_timestamp())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f81e0b5",
   "metadata": {},
   "source": [
    "# Persistencia Delta-Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b407d311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escritura del Stream a Delta Lake iniciada. Estado del Query ID: a7f9a0a5-092b-4559-8493-d34331a9647b\n",
      "El Job está corriendo. Presiona el botón de 'Stop' o interrupción del kernel en Jupyter para detenerlo.\n"
     ]
    }
   ],
   "source": [
    "# Rutas para el almacenamiento Delta Lake\n",
    "DELTA_LAKE_PATH = \"/opt/spark/data/delta/silver_contracts\"\n",
    "CHECKPOINT_PATH = \"/opt/spark/data/checkpoints/silver_contracts\"\n",
    "\n",
    "# 4. Tarea: Guardar los datos limpios en formato Delta Lake\n",
    "query = (df_silver.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") # Añadir nuevos registros\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_PATH) # Obligatorio para Spark Streaming\n",
    "    .option(\"path\", DELTA_LAKE_PATH) \n",
    "    .trigger(processingTime='10 seconds') # Procesa nuevos datos cada 10 segundos\n",
    "    .start()\n",
    ")\n",
    "\n",
    "print(f\"Escritura del Stream a Delta Lake iniciada. Estado del Query ID: {query.id}\")\n",
    "print(\"El Job está corriendo. Presiona el botón de 'Stop' o interrupción del kernel en Jupyter para detenerlo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cde49f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Esquema Final de la Tabla Silver ---\n",
      "root\n",
      " |-- id_contrato: string (nullable = true)\n",
      " |-- objeto_contrato: string (nullable = true)\n",
      " |-- entidad: string (nullable = true)\n",
      " |-- codigo_unspsc: string (nullable = true)\n",
      " |-- duracion_dias: long (nullable = true)\n",
      " |-- valor_contrato: double (nullable = true)\n",
      " |-- fecha_firma: string (nullable = true)\n",
      " |-- departamento: string (nullable = true)\n",
      " |-- kafka_ingestion_time: timestamp (nullable = true)\n",
      " |-- kafka_offset: long (nullable = true)\n",
      " |-- macrorregion_turistica: string (nullable = true)\n",
      " |-- processing_time: timestamp (nullable = true)\n",
      "\n",
      "\n",
      "--- Primeros 5 Contratos del Eje Cafetero ---\n",
      "+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------+-------------+-------------+---------------+-----------+------------+-----------------------+------------+----------------------+-----------------------+\n",
      "|id_contrato       |objeto_contrato                                                                                                                                                                                                                                                                                   |entidad                               |codigo_unspsc|duracion_dias|valor_contrato |fecha_firma|departamento|kafka_ingestion_time   |kafka_offset|macrorregion_turistica|processing_time        |\n",
      "+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------+-------------+-------------+---------------+-----------+------------+-----------------------+------------+----------------------+-----------------------+\n",
      "|CO1.PCCNTR.5948942|PRESTAR LOS SERVICIOS PROFESIONALES PARA APOYAR EL SEGUIMIENTO ADMINISTRATIVO TECNICO FINANCIERO Y LA SUPERVISION DE LOS DIFERENTES PROCESOS QUE SE\\nREQUIERAN PARA LA EJECUCION DE LOS PROYECTOS DE INVERSION ADSCRITOS A LA\\nSECRETARIA DE DESARROLLO SOCIAL Y POLITICO DEL MUNICIPIO DE PEREIRA|MUNICIPIO DE PEREIRA- OFICIAL         |V1.80111701  |0            |2.16E7         |NULL       |Risaralda   |2025-12-06 22:10:30.311|1663        |Eje Cafetero          |2025-12-06 22:10:33.182|\n",
      "|CO1.PCCNTR.7899446|No definido                                                                                                                                                                                                                                                                                       |ALCALDÍA MUNICIPAL DE EL ESPINAL      |V1.30161500  |0            |3.2126304068E10|NULL       |Tolima      |2025-12-06 22:10:30.812|1664        |Eje Cafetero          |2025-12-06 22:10:33.182|\n",
      "|CO1.PCCNTR.1769849|No definido                                                                                                                                                                                                                                                                                       |DUBERNEY MAHECHA GALINDO              |V1.85101600  |0            |0.0            |NULL       |Antioquia   |2025-12-06 22:10:31.313|1665        |Eje Cafetero          |2025-12-06 22:10:33.182|\n",
      "|CO1.PCCNTR.2349051|No definido                                                                                                                                                                                                                                                                                       |INSTITUTO TECNOLOGICO METROPOLITANO   |V1.93151507  |0            |0.0            |NULL       |Antioquia   |2025-12-06 22:10:31.819|1666        |Eje Cafetero          |2025-12-06 22:10:33.182|\n",
      "|CO1.PCCNTR.616228 |AUXILIAR DE ENFERMERIA                                                                                                                                                                                                                                                                            |REGIONAL DE ASEGURAMIENTO EN SALUD N°6|V1.85101601  |0            |6840810.0      |NULL       |Antioquia   |2025-12-06 22:10:32.319|1667        |Eje Cafetero          |2025-12-06 22:10:33.182|\n",
      "+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------+-------------+-------------+---------------+-----------+------------+-----------------------+------------+----------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Total de Contratos Guardados: 459\n",
      "+----------------------+-----+\n",
      "|macrorregion_turistica|count|\n",
      "+----------------------+-----+\n",
      "|          Eje Cafetero|  459|\n",
      "+----------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## CÓDIGO DE VERIFICACIÓN DE DELTA LAKE\n",
    "\n",
    "DELTA_LAKE_PATH = \"/opt/spark/data/delta/silver_contracts\"\n",
    "\n",
    "# 1. Leer el contenido guardado en Delta Lake\n",
    "df_silver_check = spark.read.format(\"delta\").load(DELTA_LAKE_PATH)\n",
    "\n",
    "print(\"--- Esquema Final de la Tabla Silver ---\")\n",
    "# 2. Verificar el Esquema (Solo debe tener las 8 columnas clave + metadatos)\n",
    "df_silver_check.printSchema()\n",
    "\n",
    "print(\"\\n--- Primeros 5 Contratos del Eje Cafetero ---\")\n",
    "# 3. Mostrar los primeros registros para inspección\n",
    "# Debes ver las columnas clave (id_contrato, valor_contrato, macrorregion_turistica)\n",
    "df_silver_check.show(5, truncate=False)\n",
    "\n",
    "print(f\"\\nTotal de Contratos Guardados: {df_silver_check.count()}\")\n",
    "\n",
    "# 4. Confirmación del Filtro Regional\n",
    "# Verificamos que la columna macrorregion_turistica SIEMPRE sea 'Eje Cafetero'\n",
    "df_filtered_region = df_silver_check.groupBy(\"macrorregion_turistica\").count()\n",
    "df_filtered_region.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
