{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "446da8f7",
   "metadata": {},
   "source": [
    "# Inicializaci√≥n SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baf2b2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sesi√≥n de Spark iniciada con soporte para Kafka y Delta Lake.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, broadcast, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, TimestampType\n",
    "\n",
    "# 1. INICIALIZACI√ìN DE SPARK\n",
    "# Los packages se especifican aqu√≠ para que Spark los descargue y use\n",
    "SPARK_PACKAGES = (\n",
    "    \"io.delta:delta-spark_2.12:3.1.0,\"\n",
    "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\"\n",
    ")\n",
    "SPARK_MASTER = \"spark://0.0.0.0:7077\" # Conecta al Spark Master dentro del contenedor\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BronzeToSilverStreaming\") \\\n",
    "    .master(SPARK_MASTER) \\\n",
    "    .config(\"spark.jars.packages\", SPARK_PACKAGES) \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"Sesi√≥n de Spark iniciada con soporte para Kafka y Delta Lake.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0036ca6e",
   "metadata": {},
   "source": [
    "# Esquema y lectura de Kafka (Capa Bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4963943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream de Kafka configurado.\n"
     ]
    }
   ],
   "source": [
    "# Esquema de los datos REALES del productor\n",
    "CONTRACT_SCHEMA = StructType([\n",
    "    StructField(\"id_contrato\", StringType(), True),\n",
    "    StructField(\"objeto_contrato\", StringType(), True),\n",
    "    StructField(\"entidad\", StringType(), True),\n",
    "    StructField(\"codigo_unspsc\", StringType(), True), \n",
    "    StructField(\"duracion_dias\", LongType(), True),\n",
    "    StructField(\"valor_contrato\", DoubleType(), True),\n",
    "    StructField(\"fecha_firma\", StringType(), True),\n",
    "    StructField(\"departamento\", StringType(), True),\n",
    "    \n",
    "    # Variables a ser eliminadas\n",
    "    StructField(\"sector_ruidoso\", StringType(), True), \n",
    "    StructField(\"estado_proceso_ruidoso\", StringType(), True),\n",
    "])\n",
    "# Leer el stream desde Kafka (Capa Bronze)\n",
    "kafka_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"contratos-publicos\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "print(\"Stream de Kafka configurado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d0c978",
   "metadata": {},
   "source": [
    "# Transformaci√≥n y limpieza (Bronze -> Silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7088408d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DataFrame de Regiones (Broadcast) listo para el Join.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'to_timestamp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ DataFrame de Regiones (Broadcast) listo para el Join.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# 4. APLICACI√ìN DE TRANSFORMACIONES (BRONZE -> SILVER)\u001b[39;00m\n\u001b[32m     22\u001b[39m df_silver = (\n\u001b[32m     23\u001b[39m     kafka_stream\n\u001b[32m     24\u001b[39m         \u001b[38;5;66;03m# Explosi√≥n del JSON y metadatos de Kafka\u001b[39;00m\n\u001b[32m     25\u001b[39m         .withColumn(\n\u001b[32m     26\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mvalue_content\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m             from_json(col(\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m), CONTRACT_SCHEMA)\n\u001b[32m     28\u001b[39m         )\n\u001b[32m     29\u001b[39m         .select(\n\u001b[32m     30\u001b[39m             col(\u001b[33m\"\u001b[39m\u001b[33mvalue_content.*\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     31\u001b[39m             col(\u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33mkafka_ingestion_time\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     32\u001b[39m             col(\u001b[33m\"\u001b[39m\u001b[33moffset\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33mkafka_offset\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m         )\n\u001b[32m     34\u001b[39m \n\u001b[32m     35\u001b[39m         \u001b[38;5;66;03m# LIMPIEZA: Eliminaci√≥n de columnas ruidosas\u001b[39;00m\n\u001b[32m     36\u001b[39m         .drop(\u001b[33m\"\u001b[39m\u001b[33msector_ruidoso\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mestado_proceso_ruidoso\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m \n\u001b[32m     38\u001b[39m         \u001b[38;5;66;03m# CRUCE con Regiones (solo pasa Eje Cafetero)\u001b[39;00m\n\u001b[32m     39\u001b[39m         .join(\n\u001b[32m     40\u001b[39m             df_regiones_broadcast,\n\u001b[32m     41\u001b[39m             on=df_regiones_broadcast.departamento_join == col(\u001b[33m\"\u001b[39m\u001b[33mdepartamento\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     42\u001b[39m             how=\u001b[33m\"\u001b[39m\u001b[33minner\u001b[39m\u001b[33m\"\u001b[39m   \u001b[38;5;66;03m# Asegura que solo queden departamentos del Eje Cafetero\u001b[39;00m\n\u001b[32m     43\u001b[39m         )\n\u001b[32m     44\u001b[39m         .drop(\u001b[33m\"\u001b[39m\u001b[33mdepartamento_join\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     45\u001b[39m \n\u001b[32m     46\u001b[39m         \u001b[38;5;66;03m# FILTRO expl√≠cito del eje cafetero\u001b[39;00m\n\u001b[32m     47\u001b[39m         .filter(col(\u001b[33m\"\u001b[39m\u001b[33mmacrorregion_turistica\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33mEje Cafetero\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m \n\u001b[32m     49\u001b[39m         \u001b[38;5;66;03m# üëâ FILTRO DEL A√ëO 2024\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         .withColumn(\u001b[33m\"\u001b[39m\u001b[33mfecha\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mto_timestamp\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mfecha\u001b[39m\u001b[33m\"\u001b[39m))  \u001b[38;5;66;03m# Ajusta si ya es timestamp\u001b[39;00m\n\u001b[32m     51\u001b[39m         .filter(year(col(\u001b[33m\"\u001b[39m\u001b[33mfecha\u001b[39m\u001b[33m\"\u001b[39m)) == \u001b[32m2024\u001b[39m)\n\u001b[32m     52\u001b[39m \n\u001b[32m     53\u001b[39m         \u001b[38;5;66;03m# A√±adir marca de tiempo del procesamiento\u001b[39;00m\n\u001b[32m     54\u001b[39m         .withColumn(\u001b[33m\"\u001b[39m\u001b[33mprocessing_time\u001b[39m\u001b[33m\"\u001b[39m, current_timestamp())\n\u001b[32m     55\u001b[39m )\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Pipeline (Silver) listo: limpieza, join, filtro del Eje Cafetero y filtro del a√±o 2024.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'to_timestamp' is not defined"
     ]
    }
   ],
   "source": [
    "# 3. DATOS DE CRUCE Y FILTRADO (SOLO EJE CAFETERO)\n",
    "\n",
    "# Dataset de Regiones: Solo incluye los departamentos del Eje Cafetero\n",
    "regiones_eje_cafetero = [\n",
    "    (\"ANTIOQUIA\", \"Eje Cafetero\"), \n",
    "    (\"CALDAS\", \"Eje Cafetero\"),\n",
    "    (\"QUINDIO\", \"Eje Cafetero\"), \n",
    "    (\"RISARALDA\", \"Eje Cafetero\"),\n",
    "    (\"TOLIMA\", \"Eje Cafetero\"), \n",
    "    (\"VALLE DEL CAUCA\", \"Eje Cafetero\")\n",
    "]\n",
    "\n",
    "df_regiones = (\n",
    "    spark.createDataFrame(regiones_eje_cafetero)\n",
    "         .toDF(\"departamento_join\", \"macrorregion_turistica\")\n",
    ")\n",
    "\n",
    "df_regiones_broadcast = broadcast(df_regiones)\n",
    "\n",
    "print(\"‚úÖ DataFrame de Regiones (Broadcast) listo para el Join.\")\n",
    "\n",
    "\n",
    "# 4. APLICACI√ìN DE TRANSFORMACIONES (BRONZE -> SILVER)\n",
    "\n",
    "df_silver = (\n",
    "    kafka_stream\n",
    "        # Explosi√≥n del JSON y metadatos de Kafka\n",
    "        .withColumn(\n",
    "            \"value_content\",\n",
    "            from_json(col(\"value\").cast(\"string\"), CONTRACT_SCHEMA)\n",
    "        )\n",
    "        .select(\n",
    "            col(\"value_content.*\"),\n",
    "            col(\"timestamp\").alias(\"kafka_ingestion_time\"),\n",
    "            col(\"offset\").alias(\"kafka_offset\")\n",
    "        )\n",
    "\n",
    "        # LIMPIEZA: Eliminaci√≥n de Redundantes/Ruidosas\n",
    "        .drop(\"sector_ruidoso\", \"estado_proceso_ruidoso\")\n",
    "\n",
    "        # CRUCE: Broadcasting Join con Regiones (solo Eje Cafetero)\n",
    "        .join(\n",
    "            df_regiones_broadcast,\n",
    "            on=df_regiones_broadcast.departamento_join == col(\"departamento\"),\n",
    "            how=\"inner\"  # INNER JOIN asegura que solo pasen los que coinciden\n",
    "        )\n",
    "        .drop(\"departamento_join\")\n",
    "\n",
    "        # Filtrar expl√≠citamente Eje Cafetero y a√±adir tiempo de procesamiento\n",
    "        .filter(col(\"macrorregion_turistica\") == \"Eje Cafetero\")\n",
    "        .withColumn(\"processing_time\", current_timestamp())\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Pipeline de limpieza, filtro (Eje Cafetero) y transformaci√≥n (Silver) definido.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a26204",
   "metadata": {},
   "source": [
    "# Persistencia en Delta-Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d011fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. PERSISTENCIA EN DELTA LAKE (CAPA SILVER)\n",
    "\n",
    "DELTA_LAKE_PATH = \"/opt/spark/data/delta/silver_contracts\"\n",
    "CHECKPOINT_PATH = \"/opt/spark/data/checkpoints/silver_contracts\"\n",
    "\n",
    "# Iniciar la escritura del stream\n",
    "query = (\n",
    "    df_silver.writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")   # ‚Üê aqu√≠ faltaba el backslash\n",
    "        .option(\"checkpointLocation\", CHECKPOINT_PATH)\n",
    "        .option(\"path\", DELTA_LAKE_PATH)\n",
    "        .trigger(processingTime=\"10 seconds\")\n",
    "        .start()\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Escritura del Stream a Delta Lake iniciada. Estado del Query ID: {query.id}\")\n",
    "\n",
    "# query.awaitTermination()  # Descomenta para mantener la ejecuci√≥n viva\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
